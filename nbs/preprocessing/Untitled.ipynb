{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b36776-58ac-4609-8160-ba47d876d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct, udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructField, StructType\n",
    "\n",
    "# === Parameters ===\n",
    "input_path = \"/path/to/input.parquet\"\n",
    "output_path = \"/path/to/output_scaled.parquet\"\n",
    "scaler_save_path = \"/path/to/scalers.parquet\"\n",
    "use_minmax = True  # If False, StandardScaler will be used\n",
    "\n",
    "# === Spark Session ===\n",
    "spark = SparkSession.builder.appName(\"ScalerPerGroup\").getOrCreate()\n",
    "\n",
    "# === Load Data ===\n",
    "df = spark.read.parquet(input_path).select(\"unique_id\", \"ds\", \"y\")\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Convert y to vector for scaler\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_vector = udf(lambda x: Vectors.dense([x]), VectorUDT())\n",
    "df_vec = df.withColumn(\"y_vec\", to_vector(\"y\"))\n",
    "\n",
    "# === Scale per unique_id ===\n",
    "\n",
    "\n",
    "def scale_group(pdf, scaler_type=\"minmax\"):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "    uid = pdf[\"unique_id\"].iloc[0]\n",
    "    y_vals = pdf[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "    if scaler_type == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    scaled = scaler.fit_transform(y_vals)\n",
    "    pdf[\"y_scaled\"] = scaled.flatten()\n",
    "\n",
    "    # Save scaler stats\n",
    "    scaler_info = {\n",
    "        \"unique_id\": uid,\n",
    "        \"mean\": scaler.mean_[0] if hasattr(scaler, \"mean_\") else None,\n",
    "        \"std\": scaler.scale_[0] if hasattr(scaler, \"scale_\") else None,\n",
    "        \"min\": scaler.data_min_[0] if hasattr(scaler, \"data_min_\") else None,\n",
    "        \"max\": scaler.data_max_[0] if hasattr(scaler, \"data_max_\") else None,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(pdf), pd.DataFrame([scaler_info])\n",
    "\n",
    "\n",
    "# Apply per-group scaling\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "schema_scaled = StructType(\n",
    "    [\n",
    "        StructField(\"unique_id\", StringType()),\n",
    "        StructField(\"ds\", TimestampType()),\n",
    "        StructField(\"y\", DoubleType()),\n",
    "        StructField(\"y_vec\", VectorUDT()),\n",
    "        StructField(\"y_scaled\", DoubleType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema_scaler = StructType(\n",
    "    [\n",
    "        StructField(\"unique_id\", StringType()),\n",
    "        StructField(\"mean\", DoubleType()),\n",
    "        StructField(\"std\", DoubleType()),\n",
    "        StructField(\"min\", DoubleType()),\n",
    "        StructField(\"max\", DoubleType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def grouped_scaler(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        scaled_df, scaler_info_df = scale_group(\n",
    "            pdf, scaler_type=\"minmax\" if use_minmax else \"standard\"\n",
    "        )\n",
    "        yield scaled_df\n",
    "\n",
    "\n",
    "def grouped_scaler_info(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        _, scaler_info_df = scale_group(pdf, scaler_type=\"minmax\" if use_minmax else \"standard\")\n",
    "        yield scaler_info_df\n",
    "\n",
    "\n",
    "# Apply to groups\n",
    "scaled_df = df_vec.groupBy(\"unique_id\").applyInPandas(grouped_scaler, schema=schema_scaled)\n",
    "scaler_info_df = df_vec.groupBy(\"unique_id\").applyInPandas(\n",
    "    grouped_scaler_info, schema=schema_scaler\n",
    ")\n",
    "\n",
    "# === Save Output ===\n",
    "scaled_df.select(\"unique_id\", \"ds\", \"y_scaled\").write.mode(\"overwrite\").parquet(output_path)\n",
    "scaler_info_df.write.mode(\"overwrite\").parquet(scaler_save_path)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
