{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8adef-d861-4971-9d34-5bed2e376897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp preprocess.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73e725-bd0c-49b0-8a3f-6b6e64119b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb915ed2-dc81-4b68-bde7-8e45445be373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)  # Change to DEBUG for more details\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ef341-ee5b-4dae-a904-35841cb292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f938c-14c5-4107-bf19-972d879d3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4af791-855c-40b0-9835-992b7b225cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TSRegressionDataset(Dataset):\n",
    "    def __init__(self, df, in_features, out_features, window=1):\n",
    "        super(TSRegressionDataset, self).__init__()\n",
    "\n",
    "        self.data = df\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.window = window\n",
    "\n",
    "        self.groups = {uid: group.reset_index(drop=True) for uid, group in df.groupby(\"unique_id\")}\n",
    "\n",
    "    def __len__(self):\n",
    "        assert self.in_features + self.out_features < len(\n",
    "            self.data\n",
    "        ), f\"in_features + out_features should not be greater than series len {self.data.shape[0]}\"\n",
    "\n",
    "        return sum(\n",
    "            [\n",
    "                (len(g) - (self.in_features + self.out_features)) // self.window + 1\n",
    "                for g in self.groups.values()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        for uid, df in self.groups.items():\n",
    "            max_idx = (len(df) - (self.in_features + self.out_features)) // self.window + 1\n",
    "\n",
    "            if idx < max_idx:\n",
    "                start = idx * self.window\n",
    "                x = torch.tensor(\n",
    "                    df[\"y\"].iloc[start : start + self.in_features].values,\n",
    "                    dtype=torch.float32,  # device=torch.device(\"cuda\")\n",
    "                )\n",
    "                y = torch.tensor(\n",
    "                    df[\"y\"]\n",
    "                    .iloc[start + self.in_features : start + self.in_features + self.out_features]\n",
    "                    .values,\n",
    "                    dtype=torch.float32,  # device=torch.device(\"cuda\")\n",
    "                )\n",
    "                return x, y\n",
    "\n",
    "            idx -= max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0712d0-9318-454a-89b8-ba2c0f4b957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TSDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, df, in_features, out_features, window=1, batch_size=32):\n",
    "        super(TSDataLoader, self).__init__()\n",
    "        self.data = df\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.window = window\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        assert list(self.data.columns) == [\n",
    "            \"unique_id\",\n",
    "            \"ds\",\n",
    "            \"y\",\n",
    "        ], \"Columns must be ['unique_id', 'ds', 'y']\"\n",
    "\n",
    "        self.data = self.data.sort_values([\"unique_id\", \"ds\"])\n",
    "\n",
    "        # Train / Val / Test Split (70/15/15)\n",
    "        train_sz = int(len(self.data) * 0.7)\n",
    "        val_sz = int(len(self.data) * 0.15)\n",
    "\n",
    "        self.train_df = self.data.iloc[:train_sz]\n",
    "        self.val_df = self.data.iloc[train_sz : train_sz + val_sz]\n",
    "        self.test_df = self.data.iloc[train_sz + val_sz :]\n",
    "\n",
    "        self.train = TSRegressionDataset(\n",
    "            self.train_df, self.in_features, self.out_features, self.window\n",
    "        )\n",
    "        self.val = TSRegressionDataset(\n",
    "            self.val_df, self.in_features, self.out_features, self.window\n",
    "        )\n",
    "        self.test = TSRegressionDataset(\n",
    "            self.test_df, self.in_features, self.out_features, self.window\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=32,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=32,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=32,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f31bd-3f43-4c3e-b758-a01bd4336713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12]) torch.Size([32, 6])\n",
      "torch.Size([32, 12]) torch.Size([32, 6])\n",
      "torch.Size([19, 12]) torch.Size([19, 6])\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\":\n",
    "    from neuralforecast.utils import AirPassengersDF as df\n",
    "\n",
    "    in_features, out_features, window, batch_sz = 12, 6, 1, 32\n",
    "    ds = TSDataLoader(df, in_features, out_features, window, batch_sz)\n",
    "    ds.setup()\n",
    "    for features, labels in ds.train_dataloader():\n",
    "        print(features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a9283-fb72-4c3f-82df-2ac01743be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class UnivariateTSDataset(Dataset):\n",
    "    def __init__(self, windows):\n",
    "        self.windows = windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.windows[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class UnivariateTSDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        batch_size=32,\n",
    "        num_workers=12,\n",
    "        train_split=0.7,\n",
    "        val_split=0.15,\n",
    "        normalize=True,\n",
    "        scaler_type=\"minmax\",\n",
    "        split_type=\"horizontal\",  # New: \"horizontal\" or \"vertical\"\n",
    "        step_size=1,  # New: Step size for sliding window\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with ['unique_id', 'ds', 'y'].\n",
    "            input_size (int): Length of the lookback window.\n",
    "            horizon (int): Length of the forecast horizon.\n",
    "            batch_size (int): Batch size for DataLoader.\n",
    "            train_split (float): Fraction for training.\n",
    "            val_split (float): Fraction for validation (rest is test).\n",
    "            normalize (bool): Whether to normalize data.\n",
    "            scaler_type (str): 'minmax' or 'standard'.\n",
    "            split_type (str): 'horizontal' (split within series) or 'vertical' (split by unique_id).\n",
    "            step_size (int): Step size for sliding windows (1 = no skip, >1 = skip points).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.normalize = normalize\n",
    "        self.scaler_type = scaler_type\n",
    "        self.split_type = split_type\n",
    "        self.step_size = step_size\n",
    "        self.scalers = {}\n",
    "\n",
    "        # Validate splits\n",
    "        if not 0 < train_split + val_split <= 1:\n",
    "            raise ValueError(\"train_split + val_split must be between 0 and 1\")\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must be >= 1\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        grouped = self.df.groupby(\"unique_id\")\n",
    "        all_series = {unique_id: group[\"y\"].values for unique_id, group in grouped}\n",
    "\n",
    "        # Normalize series if requested\n",
    "        if self.normalize:\n",
    "            if self.scaler_type == \"minmax\":\n",
    "                scaler_class = MinMaxScaler\n",
    "            elif self.scaler_type == \"standard\":\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "                scaler_class = StandardScaler\n",
    "            else:\n",
    "                raise ValueError(\"scaler_type must be 'minmax' or 'standard'\")\n",
    "\n",
    "            normalized_series = {}\n",
    "            for unique_id, series in all_series.items():\n",
    "                scaler = scaler_class()\n",
    "                normalized = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "                self.scalers[unique_id] = scaler\n",
    "                normalized_series[unique_id] = normalized\n",
    "        else:\n",
    "            normalized_series = all_series\n",
    "\n",
    "        train_windows, val_windows, test_windows = [], [], []\n",
    "\n",
    "        if self.split_type == \"horizontal\":\n",
    "            # Horizontal split: Split each series into train/val/test segments\n",
    "            for unique_id, series in normalized_series.items():\n",
    "                series_len = len(series)\n",
    "                if series_len < self.input_size + self.horizon:\n",
    "                    logger.warning(\n",
    "                        f\"{unique_id} - Series length {series_len} is too short for input_size={self.input_size} + horizon={self.horizon}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                max_idx = series_len - self.input_size - self.horizon + 1\n",
    "                if max_idx <= 0:\n",
    "                    logger.warning(f\"{unique_id} - No valid windows possible\")\n",
    "                    continue\n",
    "\n",
    "                # Adjust max_idx based on step_size\n",
    "                num_windows = (max_idx - 1) // self.step_size + 1\n",
    "                train_end = int(num_windows * self.train_split)\n",
    "                val_end = train_end + int(num_windows * self.val_split)\n",
    "\n",
    "                for j in range(0, max_idx, self.step_size):\n",
    "                    if j + self.input_size + self.horizon > series_len:\n",
    "                        break  # Ensure window doesn’t exceed series length\n",
    "                    x = series[j : j + self.input_size]\n",
    "                    y = series[j + self.input_size : j + self.input_size + self.horizon]\n",
    "                    window = (x, y)\n",
    "                    window_idx = j // self.step_size\n",
    "                    if window_idx < train_end:\n",
    "                        train_windows.append(window)\n",
    "                    elif window_idx < val_end:\n",
    "                        val_windows.append(window)\n",
    "                    else:\n",
    "                        test_windows.append(window)\n",
    "\n",
    "        elif self.split_type == \"vertical\":\n",
    "            # Vertical split: Split by unique_id\n",
    "            unique_ids = list(normalized_series.keys())\n",
    "            np.random.shuffle(unique_ids)  # Randomly shuffle series\n",
    "            total_series = len(unique_ids)\n",
    "            train_end = int(total_series * self.train_split)\n",
    "            val_end = train_end + int(total_series * self.val_split)\n",
    "\n",
    "            train_ids = unique_ids[:train_end]\n",
    "            val_ids = unique_ids[train_end:val_end]\n",
    "            test_ids = unique_ids[val_end:]\n",
    "\n",
    "            for unique_id in train_ids:\n",
    "                series = normalized_series[unique_id]\n",
    "                series_len = len(series)\n",
    "                if series_len < self.input_size + self.horizon:\n",
    "                    logger.warning(f\"{unique_id} - Series length {series_len} is too short\")\n",
    "                    continue\n",
    "\n",
    "                max_idx = series_len - self.input_size - self.horizon + 1\n",
    "                if max_idx <= 0:\n",
    "                    continue\n",
    "\n",
    "                for j in range(0, max_idx, self.step_size):\n",
    "                    if j + self.input_size + self.horizon > series_len:\n",
    "                        break\n",
    "                    x = series[j : j + self.input_size]\n",
    "                    y = series[j + self.input_size : j + self.input_size + self.horizon]\n",
    "                    train_windows.append((x, y))\n",
    "\n",
    "            for unique_id in val_ids:\n",
    "                series = normalized_series[unique_id]\n",
    "                series_len = len(series)\n",
    "                if series_len < self.input_size + self.horizon:\n",
    "                    logger.warning(f\"{unique_id} - Series length {series_len} is too short\")\n",
    "                    continue\n",
    "\n",
    "                max_idx = series_len - self.input_size - self.horizon + 1\n",
    "                if max_idx <= 0:\n",
    "                    continue\n",
    "\n",
    "                for j in range(0, max_idx, self.step_size):\n",
    "                    if j + self.input_size + self.horizon > series_len:\n",
    "                        break\n",
    "                    x = series[j : j + self.input_size]\n",
    "                    y = series[j + self.input_size : j + self.input_size + self.horizon]\n",
    "                    val_windows.append((x, y))\n",
    "\n",
    "            for unique_id in test_ids:\n",
    "                series = normalized_series[unique_id]\n",
    "                series_len = len(series)\n",
    "                if series_len < self.input_size + self.horizon:\n",
    "                    logger.warning(f\"{unique_id} - Series length {series_len} is too short\")\n",
    "                    continue\n",
    "\n",
    "                max_idx = series_len - self.input_size - self.horizon + 1\n",
    "                if max_idx <= 0:\n",
    "                    continue\n",
    "\n",
    "                for j in range(0, max_idx, self.step_size):\n",
    "                    if j + self.input_size + self.horizon > series_len:\n",
    "                        break\n",
    "                    x = series[j : j + self.input_size]\n",
    "                    y = series[j + self.input_size : j + self.input_size + self.horizon]\n",
    "                    test_windows.append((x, y))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"split_type must be 'horizontal' or 'vertical'\")\n",
    "\n",
    "        self.train_dataset = UnivariateTSDataset(train_windows)\n",
    "        self.val_dataset = UnivariateTSDataset(val_windows)\n",
    "        self.test_dataset = UnivariateTSDataset(test_windows)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Train windows: {len(train_windows)}, Val windows: {len(val_windows)}, Test windows: {len(test_windows)}\"\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def inverse_transform(self, data, unique_id):\n",
    "        if self.normalize and unique_id in self.scalers:\n",
    "            return self.scalers[unique_id].inverse_transform(data.reshape(-1, 1)).flatten()\n",
    "        return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
