{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe8adef-d861-4971-9d34-5bed2e376897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp preprocess.bigdata.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7deab329-bf09-40ce-8f13-e21abfa83665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pytorch_lightning as pl\n",
    "import ray\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "\n",
    "class HDF5Writer:\n",
    "    \"\"\"Manages thread-safe appending to a single HDF5 file.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, input_size, horizon):\n",
    "        self.file_path = Path(file_path)\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.file = None\n",
    "        self.x_dset = None\n",
    "        self.y_dset = None\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def open(self):\n",
    "        if self.file is None:\n",
    "            self.file = h5py.File(self.file_path, \"a\")\n",
    "            self.x_dset = self.file.get(\"x\")\n",
    "            self.y_dset = self.file.get(\"y\")\n",
    "            if self.x_dset is None:\n",
    "                self.x_dset = self.file.create_dataset(\n",
    "                    \"x\",\n",
    "                    shape=(0, self.input_size),\n",
    "                    maxshape=(None, self.input_size),\n",
    "                    dtype=np.float32,\n",
    "                    compression=\"lzf\",\n",
    "                )\n",
    "                self.y_dset = self.file.create_dataset(\n",
    "                    \"y\",\n",
    "                    shape=(0, self.horizon),\n",
    "                    maxshape=(None, self.horizon),\n",
    "                    dtype=np.float32,\n",
    "                    compression=\"lzf\",\n",
    "                )\n",
    "            self.current_idx = self.x_dset.shape[0]\n",
    "\n",
    "    def append(self, x_windows, y_windows):\n",
    "        self.open()\n",
    "        num_windows = len(x_windows)\n",
    "        self.x_dset.resize(self.current_idx + num_windows, axis=0)\n",
    "        self.y_dset.resize(self.current_idx + num_windows, axis=0)\n",
    "        self.x_dset[self.current_idx : self.current_idx + num_windows] = x_windows\n",
    "        self.y_dset[self.current_idx : self.current_idx + num_windows] = y_windows\n",
    "        self.current_idx += num_windows\n",
    "        self.file.flush()\n",
    "        logger.debug(f\"Appended {num_windows} windows to {self.file_path}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "            self.file = None\n",
    "            self.x_dset = None\n",
    "            self.y_dset = None\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class HDF5WriterActor:\n",
    "    def __init__(self, file_path, input_size, horizon):\n",
    "        self.writer = HDF5Writer(file_path, input_size, horizon)\n",
    "\n",
    "    def append(self, x_windows, y_windows):\n",
    "        self.writer.append(x_windows, y_windows)\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "\n",
    "class TSPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        target_col=\"y_scaled\",\n",
    "        train_split=0.7,\n",
    "        val_split=0.15,\n",
    "        split_type=\"horizontal\",\n",
    "        step_size=1,\n",
    "        cache_dir=\".\",\n",
    "        use_cache=True,\n",
    "        experiment_name=\"default_experiment\",\n",
    "        num_workers=4,\n",
    "        chunk_size=2000,\n",
    "        adaptive_step=False,\n",
    "        downsample_factor=1,\n",
    "        large_dataset=True,\n",
    "        save_by_window_count=False,\n",
    "        max_windows_per_file=2000,\n",
    "        batch_size=200,\n",
    "    ):\n",
    "        logger.info(\"Initializing TSPreprocessor\")\n",
    "        self.df = df\n",
    "        self.target_col = target_col if target_col in df.columns else \"y\"\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.split_type = split_type\n",
    "        self.step_size = step_size\n",
    "        self.cache_dir = Path(cache_dir) / experiment_name\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.use_cache = use_cache\n",
    "        self.num_workers = min(num_workers, os.cpu_count() or 4)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.adaptive_step = adaptive_step\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.large_dataset = large_dataset\n",
    "        self.save_by_window_count = save_by_window_count\n",
    "        self.max_windows_per_file = max_windows_per_file\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if not 0 < train_split + val_split <= 1:\n",
    "            raise ValueError(\"train_split + val_split must be between 0 and 1\")\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must be >= 1\")\n",
    "        if downsample_factor < 1:\n",
    "            raise ValueError(\"downsample_factor must be >= 1\")\n",
    "        if max_windows_per_file < 1:\n",
    "            raise ValueError(\"max_windows_per_file must be >= 1\")\n",
    "        if batch_size < 1:\n",
    "            raise ValueError(\"batch_size must be >= 1\")\n",
    "\n",
    "        # Initialize Ray\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(num_cpus=self.num_workers, ignore_reinit_error=True)\n",
    "\n",
    "        self.train_windows, self.val_windows, self.test_windows = self._process_data()\n",
    "\n",
    "        if len(self.train_windows) == 0:\n",
    "            logger.error(\n",
    "                \"No training windows generated. Check input_size, horizon, or data lengths.\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                \"Training dataset is empty. Ensure series lengths are sufficient for input_size + horizon.\"\n",
    "            )\n",
    "\n",
    "        del self.df\n",
    "        gc.collect()\n",
    "\n",
    "    def _generate_windows_in_memory(self, series):\n",
    "        series_len = len(series)\n",
    "        if series_len < self.input_size + self.horizon:\n",
    "            return []\n",
    "\n",
    "        max_idx = series_len - self.input_size - self.horizon + 1\n",
    "        if max_idx <= 0:\n",
    "            return []\n",
    "\n",
    "        window_starts = np.arange(0, max_idx, self.step_size, dtype=np.int32)\n",
    "        window_ends = window_starts + self.input_size\n",
    "        horizon_ends = window_ends + self.horizon\n",
    "\n",
    "        valid_windows = horizon_ends <= series_len\n",
    "        window_starts = window_starts[valid_windows]\n",
    "        window_ends = window_ends[valid_windows]\n",
    "        horizon_ends = horizon_ends[valid_windows]\n",
    "\n",
    "        x_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=self.input_size)[\n",
    "            window_starts\n",
    "        ]\n",
    "        y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])\n",
    "\n",
    "        return list(zip(x_windows, y_windows))\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_windows_optimized(\n",
    "        series,\n",
    "        unique_id,\n",
    "        split,\n",
    "        writer_actor,\n",
    "        chunk_size,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        step_size,\n",
    "        adaptive_step,\n",
    "    ):\n",
    "        series_len = len(series)\n",
    "        if series_len < input_size + horizon:\n",
    "            logger.debug(\n",
    "                f\"[{unique_id}] Skipping series for {split} split (length={series_len} < {input_size + horizon})\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "        max_idx = series_len - input_size - horizon + 1\n",
    "        if max_idx <= 0:\n",
    "            logger.debug(f\"[{unique_id}] No valid windows for {split} split (max_idx={max_idx})\")\n",
    "            return 0\n",
    "\n",
    "        step_size = step_size\n",
    "        if adaptive_step:\n",
    "            step_size = max(1, series_len // 1000)\n",
    "\n",
    "        chunk_size = chunk_size or max(100, min(1000, series_len // 10))\n",
    "        window_count = 0\n",
    "\n",
    "        for start in range(0, max_idx, chunk_size * step_size):\n",
    "            end = min(start + chunk_size * step_size, max_idx)\n",
    "            window_starts = np.arange(start, end, step_size, dtype=np.int32)\n",
    "            window_ends = window_starts + input_size\n",
    "            horizon_ends = window_ends + horizon\n",
    "\n",
    "            valid_windows = horizon_ends <= series_len\n",
    "            window_starts = window_starts[valid_windows]\n",
    "            window_ends = window_ends[valid_windows]\n",
    "            horizon_ends = horizon_ends[valid_windows]\n",
    "\n",
    "            x_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=input_size)[\n",
    "                window_starts\n",
    "            ]\n",
    "            y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])\n",
    "\n",
    "            ray.get(writer_actor.append.remote(x_windows, y_windows))\n",
    "            window_count += len(x_windows)\n",
    "            logger.debug(f\"[{unique_id}] Appended {len(x_windows)} windows for {split} split\")\n",
    "\n",
    "            del x_windows, y_windows\n",
    "            gc.collect()\n",
    "\n",
    "        logger.debug(f\"[{unique_id}] Total {window_count} windows appended for {split} split\")\n",
    "        return window_count\n",
    "\n",
    "    def _process_one_series_in_memory(self, unique_id_group):\n",
    "        unique_id, group = unique_id_group\n",
    "        series = group[self.target_col].values.astype(np.float32)\n",
    "\n",
    "        windows = self._generate_windows_in_memory(series)\n",
    "        del series\n",
    "        gc.collect()\n",
    "\n",
    "        if not windows:\n",
    "            logger.warning(f\"[{unique_id}] Series too short to generate any windows\")\n",
    "            return [], [], [], unique_id\n",
    "\n",
    "        if self.split_type == \"horizontal\":\n",
    "            num_windows = len(windows)\n",
    "            train_end = int(num_windows * self.train_split)\n",
    "            val_end = train_end + int(num_windows * self.val_split)\n",
    "            return (windows[:train_end], windows[train_end:val_end], windows[val_end:], unique_id)\n",
    "        else:\n",
    "            return windows, [], [], unique_id\n",
    "\n",
    "    @staticmethod\n",
    "    @ray.remote(num_cpus=1)\n",
    "    def _process_batch_optimized(\n",
    "        unique_id_groups,\n",
    "        writer_actors,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        step_size,\n",
    "        adaptive_step,\n",
    "        downsample_factor,\n",
    "        chunk_size,\n",
    "        split_type,\n",
    "        train_split,\n",
    "        val_split,\n",
    "        target_col,\n",
    "    ):\n",
    "        results = []\n",
    "        for unique_id_group in unique_id_groups:\n",
    "            unique_id, group = unique_id_group\n",
    "            series = group[target_col].values.astype(np.float32)\n",
    "\n",
    "            if len(series) < input_size + horizon:\n",
    "                logger.debug(\n",
    "                    f\"[{unique_id}] Skipping series (length={len(series)} < {input_size + horizon})\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            if downsample_factor > 1:\n",
    "                series = series[::downsample_factor]\n",
    "\n",
    "            step_size = step_size\n",
    "            if adaptive_step:\n",
    "                step_size = max(1, len(series) // 1000)\n",
    "\n",
    "            if split_type == \"horizontal\":\n",
    "                series_len = len(series)\n",
    "                num_windows = max(0, (series_len - input_size - horizon + 1) // step_size)\n",
    "                train_end = int(num_windows * train_split)\n",
    "                val_end = train_end + int(num_windows * val_split)\n",
    "\n",
    "                result = []\n",
    "                for split, start, end in [\n",
    "                    (\"train\", 0, train_end * step_size),\n",
    "                    (\"val\", train_end * step_size, val_end * step_size),\n",
    "                    (\"test\", val_end * step_size, series_len),\n",
    "                ]:\n",
    "                    window_count = TSPreprocessor._generate_windows_optimized(\n",
    "                        series[start:end],\n",
    "                        unique_id,\n",
    "                        split,\n",
    "                        writer_actors[split],\n",
    "                        chunk_size,\n",
    "                        input_size,\n",
    "                        horizon,\n",
    "                        step_size,\n",
    "                        adaptive_step,\n",
    "                    )\n",
    "                    result.append((window_count, unique_id, split))\n",
    "                results.append(result)\n",
    "            else:\n",
    "                window_count = TSPreprocessor._generate_windows_optimized(\n",
    "                    series,\n",
    "                    unique_id,\n",
    "                    \"all\",\n",
    "                    writer_actors[\"all\"],\n",
    "                    chunk_size,\n",
    "                    input_size,\n",
    "                    horizon,\n",
    "                    step_size,\n",
    "                    adaptive_step,\n",
    "                )\n",
    "                result = [(window_count, unique_id, \"all\")]\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "    def _process_data(self):\n",
    "        logger.info(\"Processing data for all unique_ids\")\n",
    "        cache_file = self.cache_dir / \"preprocessed_windows.pkl\"\n",
    "\n",
    "        if self.use_cache and cache_file.exists():\n",
    "            logger.info(\"Loading preprocessed windows from cache\")\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            return data[\"train_windows\"], data[\"val_windows\"], data[\"test_windows\"]\n",
    "\n",
    "        grouped = list(self.df.groupby(\"unique_id\"))\n",
    "        logger.info(f\"Found {len(grouped)} unique IDs\")\n",
    "\n",
    "        # Pre-filter series that are too short\n",
    "        valid_grouped = []\n",
    "        lengths = []\n",
    "        for unique_id, group in grouped:\n",
    "            series_len = len(group[self.target_col])\n",
    "            lengths.append(series_len)\n",
    "            if series_len >= self.input_size + self.horizon:\n",
    "                valid_grouped.append((unique_id, group))\n",
    "            else:\n",
    "                logger.debug(\n",
    "                    f\"[{unique_id}] Excluded: series length {series_len} < {self.input_size + self.horizon}\"\n",
    "                )\n",
    "        logger.info(f\"After filtering, {len(valid_grouped)} valid series remain\")\n",
    "        if lengths:\n",
    "            lengths = np.array(lengths)\n",
    "            logger.info(\n",
    "                f\"Series length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.2f}, \"\n",
    "                f\"median={np.median(lengths):.2f}, too_short={np.sum(lengths < self.input_size + self.horizon)}\"\n",
    "            )\n",
    "\n",
    "        if not valid_grouped:\n",
    "            logger.error(\"No series are long enough to generate windows.\")\n",
    "            raise ValueError(\"No valid series found for window generation.\")\n",
    "\n",
    "        train_windows, val_windows, test_windows = [], [], []\n",
    "        train_ids, val_ids, test_ids = [], [], []\n",
    "\n",
    "        if self.split_type == \"vertical\":\n",
    "            logger.info(\"Applying vertical split\")\n",
    "            unique_ids = [unique_id for unique_id, _ in valid_grouped]\n",
    "            np.random.shuffle(unique_ids)\n",
    "            total_series = len(unique_ids)\n",
    "            train_end = int(total_series * self.train_split)\n",
    "            val_end = train_end + int(total_series * self.val_split)\n",
    "\n",
    "            train_ids = unique_ids[:train_end]\n",
    "            val_ids = unique_ids[train_end:val_end]\n",
    "            test_ids = unique_ids[val_end:]\n",
    "            logger.info(\n",
    "                f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}\"\n",
    "            )\n",
    "\n",
    "        if self.large_dataset:\n",
    "            # Initialize HDF5 writer actors\n",
    "            writer_actors = {\n",
    "                \"train\": HDF5WriterActor.remote(\n",
    "                    self.cache_dir / \"train_windows.h5\", self.input_size, self.horizon\n",
    "                ),\n",
    "                \"val\": HDF5WriterActor.remote(\n",
    "                    self.cache_dir / \"val_windows.h5\", self.input_size, self.horizon\n",
    "                ),\n",
    "                \"test\": HDF5WriterActor.remote(\n",
    "                    self.cache_dir / \"test_windows.h5\", self.input_size, self.horizon\n",
    "                ),\n",
    "                \"all\": HDF5WriterActor.remote(\n",
    "                    self.cache_dir / \"all_windows.h5\", self.input_size, self.horizon\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Batch unique IDs\n",
    "            batches = [\n",
    "                valid_grouped[i : i + self.batch_size]\n",
    "                for i in range(0, len(valid_grouped), self.batch_size)\n",
    "            ]\n",
    "            logger.debug(f\"Created {len(batches)} batches\")\n",
    "\n",
    "            # Submit Ray tasks\n",
    "            futures = [\n",
    "                TSPreprocessor._process_batch_optimized.remote(\n",
    "                    batch,\n",
    "                    writer_actors,\n",
    "                    self.input_size,\n",
    "                    self.horizon,\n",
    "                    self.step_size,\n",
    "                    self.adaptive_step,\n",
    "                    self.downsample_factor,\n",
    "                    self.chunk_size,\n",
    "                    self.split_type,\n",
    "                    self.train_split,\n",
    "                    self.val_split,\n",
    "                    self.target_col,\n",
    "                )\n",
    "                for batch in batches\n",
    "            ]\n",
    "            logger.debug(f\"Submitted {len(futures)} Ray tasks\")\n",
    "\n",
    "            # Collect results\n",
    "            pbar = tqdm(total=len(futures), desc=\"Processing batches\", dynamic_ncols=True)\n",
    "            total_windows = {\"train\": 0, \"val\": 0, \"test\": 0, \"all\": 0}\n",
    "            while futures:\n",
    "                done, futures = ray.wait(futures, num_returns=min(len(futures), 1))\n",
    "                batch_results = ray.get(done)[0]\n",
    "                for result in batch_results:\n",
    "                    for window_count, unique_id, split in result:\n",
    "                        if window_count > 0:\n",
    "                            total_windows[split] += window_count\n",
    "                            if self.split_type == \"horizontal\":\n",
    "                                if split == \"train\":\n",
    "                                    train_windows.append(\n",
    "                                        (self.cache_dir / \"train_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                                elif split == \"val\":\n",
    "                                    val_windows.append(\n",
    "                                        (self.cache_dir / \"val_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                                elif split == \"test\":\n",
    "                                    test_windows.append(\n",
    "                                        (self.cache_dir / \"test_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                            else:\n",
    "                                if unique_id in train_ids:\n",
    "                                    train_windows.append(\n",
    "                                        (self.cache_dir / \"all_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                                elif unique_id in val_ids:\n",
    "                                    val_windows.append(\n",
    "                                        (self.cache_dir / \"all_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                                elif unique_id in test_ids:\n",
    "                                    test_windows.append(\n",
    "                                        (self.cache_dir / \"all_windows.h5\", unique_id)\n",
    "                                    )\n",
    "                pbar.update(len(done))\n",
    "            pbar.close()\n",
    "\n",
    "            # Close writer actors\n",
    "            for actor in writer_actors.values():\n",
    "                ray.get(actor.close.remote())\n",
    "\n",
    "            logger.info(\n",
    "                f\"Total windows: Train={total_windows['train']}, Val={total_windows['val']}, Test={total_windows['test']}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                results = list(\n",
    "                    tqdm(\n",
    "                        executor.map(self._process_one_series_in_memory, valid_grouped),\n",
    "                        total=len(valid_grouped),\n",
    "                        desc=\"Processing series\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            for train_w, val_w, test_w, unique_id in results:\n",
    "                if self.split_type == \"horizontal\":\n",
    "                    train_windows.extend(train_w)\n",
    "                    val_windows.extend(val_w)\n",
    "                    test_windows.extend(test_w)\n",
    "                else:\n",
    "                    if unique_id in train_ids:\n",
    "                        train_windows.extend(train_w)\n",
    "                    elif unique_id in val_ids:\n",
    "                        val_windows.extend(val_w)\n",
    "                    elif unique_id in test_ids:\n",
    "                        test_windows.extend(test_w)\n",
    "\n",
    "        train_windows = np.array(train_windows, dtype=object)\n",
    "        val_windows = np.array(val_windows, dtype=object)\n",
    "        test_windows = np.array(test_windows, dtype=object)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Train windows: {len(train_windows)}, Val windows: {len(val_windows)}, Test windows: {len(test_windows)}\"\n",
    "        )\n",
    "\n",
    "        if len(train_windows) == 0:\n",
    "            logger.error(\n",
    "                \"No training windows generated after processing. Check split_type, train_split, or data.\"\n",
    "            )\n",
    "            raise ValueError(\"Training dataset is empty. Adjust parameters or verify data.\")\n",
    "\n",
    "        if self.use_cache:\n",
    "            logger.info(\"Saving preprocessed windows metadata to cache\")\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"train_windows\": train_windows,\n",
    "                        \"val_windows\": val_windows,\n",
    "                        \"test_windows\": test_windows,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "        return train_windows, val_windows, test_windows\n",
    "\n",
    "\n",
    "class UnivariateTSDataset(Dataset):\n",
    "    def __init__(self, windows, device: str = None, large_dataset: bool = True):\n",
    "        logger.info(\"Initializing UnivariateTSDataset\")\n",
    "        self.windows = windows\n",
    "        self.device = device\n",
    "        self.large_dataset = large_dataset\n",
    "        if large_dataset:\n",
    "            # Group by HDF5 file to avoid redundant opens\n",
    "            self.h5_file_map = {}\n",
    "            for h5_file, _ in windows:\n",
    "                h5_file = str(h5_file)  # Ensure string path\n",
    "                if h5_file not in self.h5_file_map:\n",
    "                    with h5py.File(h5_file, \"r\") as f:\n",
    "                        self.h5_file_map[h5_file] = {\"size\": f[\"x\"].shape[0], \"offset\": 0}\n",
    "            # Compute cumulative offsets\n",
    "            current_offset = 0\n",
    "            for h5_file in self.h5_file_map:\n",
    "                self.h5_file_map[h5_file][\"offset\"] = current_offset\n",
    "                current_offset += self.h5_file_map[h5_file][\"size\"]\n",
    "        else:\n",
    "            self.x = np.stack([w[0] for w in windows], axis=0).astype(np.float32)\n",
    "            self.y = np.stack([w[1] for w in windows], axis=0).astype(np.float32)\n",
    "            if device:\n",
    "                logger.info(\"Preloading tensors to device: %s\", device)\n",
    "                self.x_tensor = torch.from_numpy(self.x).to(device)\n",
    "                self.y_tensor = torch.from_numpy(self.y).to(device)\n",
    "            else:\n",
    "                self.x_tensor = self.y_tensor = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.large_dataset:\n",
    "            return sum(info[\"size\"] for info in self.h5_file_map.values())\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = time.time()\n",
    "        mem_before = psutil.virtual_memory().used / 1024**3\n",
    "        if self.large_dataset:\n",
    "            # Find the HDF5 file and local index\n",
    "            for h5_file, info in self.h5_file_map.items():\n",
    "                if idx < info[\"offset\"] + info[\"size\"]:\n",
    "                    local_idx = idx - info[\"offset\"]\n",
    "                    with h5py.File(h5_file, \"r\") as f:\n",
    "                        x = f[\"x\"][local_idx]\n",
    "                        y = f[\"y\"][local_idx]\n",
    "                        x_tensor = torch.from_numpy(x)\n",
    "                        y_tensor = torch.from_numpy(y)\n",
    "                        if self.device:\n",
    "                            x_tensor = x_tensor.to(self.device)\n",
    "                            y_tensor = y_tensor.to(self.device)\n",
    "                            if self.device == \"cuda\":\n",
    "                                torch.cuda.empty_cache()\n",
    "                        mem_after = psutil.virtual_memory().used / 1024**3\n",
    "                        # logger.info(\n",
    "                        #     f\"Loaded batch idx={idx}, time={time.time() - start_time:.2f}s, \"\n",
    "                        #     f\"RAM delta={mem_after - mem_before:.2f}GB, \"\n",
    "                        #     f\"x.shape={x_tensor.shape}, y.shape={y_tensor.shape}\"\n",
    "                        # )\n",
    "                        gc.collect()\n",
    "                        return x_tensor, y_tensor\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        else:\n",
    "            x_tensor = self.x_tensor[idx] if self.device else torch.from_numpy(self.x[idx])\n",
    "            y_tensor = self.y_tensor[idx] if self.device else torch.from_numpy(self.y[idx])\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            mem_after = psutil.virtual_memory().used / 1024**3\n",
    "            # logger.info(\n",
    "            #     f\"Loaded batch idx={idx}, time={time.time() - start_time:.2f}s, \"\n",
    "            #     f\"RAM delta={mem_after - mem_before:.2f}GB, \"\n",
    "            #     f\"x.shape={x_tensor.shape}, y.shape={y_tensor.shape}\"\n",
    "            # )\n",
    "            gc.collect()\n",
    "            return x_tensor, y_tensor\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.large_dataset:\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "class UnivariateTSDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor: TSPreprocessor,\n",
    "        batch_size=32,\n",
    "        num_workers=1,\n",
    "        pin_memory=False,\n",
    "        prefetch_factor=1,\n",
    "        persistent_workers=False,\n",
    "        gpu_preload=False,\n",
    "        experiment_name=\"default_experiment\",\n",
    "    ):\n",
    "        logger.info(\"Initializing UnivariateTSDataModule\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"preprocessor\"])\n",
    "        self.preprocessor = preprocessor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = min(num_workers, torch.get_num_threads())\n",
    "        self.pin_memory = pin_memory and not gpu_preload\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.gpu_preload = gpu_preload\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        logger.info(f\"Setting up datamodule for stage: {stage}\")\n",
    "        if stage in (\"fit\", None):\n",
    "            self.train_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.train_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "            self.val_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.val_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "        if stage in (\"validate\", None):\n",
    "            self.val_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.val_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "        if stage in (\"test\", None):\n",
    "            self.test_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.test_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "\n",
    "    def _create_dataloader(self, dataset, shuffle=False):\n",
    "        logger.info(\"Creating dataloader\")\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            prefetch_factor=self.prefetch_factor,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            drop_last=shuffle,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        logger.info(\"Getting train dataloader\")\n",
    "        return self._create_dataloader(self.train_dataset, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        logger.info(\"Getting val dataloader\")\n",
    "        return self._create_dataloader(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        logger.info(\"Getting test dataloader\")\n",
    "        return self._create_dataloader(self.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874e5769-fc47-4034-b95d-3d7b36a762a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing TSPreprocessor\n",
      "INFO:__main__:Processing data for all unique_ids\n",
      "INFO:__main__:Loading preprocessed windows from cache\n",
      "INFO:__main__:Initializing UnivariateTSDataModule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 1.41 s, total: 3.21 s\n",
      "Wall time: 3.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/projects/ts/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example usage\n",
    "## Data loading and preprocessing df.columns - unique_id,ds,y\n",
    "from datasetsforecast.m5 import M5\n",
    "\n",
    "df = M5().load(\"../data\")[0]  # , group=\"Monthly\"\n",
    "df.sort_values([\"unique_id\", \"ds\"], inplace=True)\n",
    "\n",
    "\n",
    "horizon = 180  # <-- FORECAST HORIZON\n",
    "input_size = horizon * 5\n",
    "\n",
    "batch_size = 64\n",
    "step_size = 7\n",
    "\n",
    "# ----------\n",
    "# from datasetsforecast.m3 import M3\n",
    "\n",
    "# df = M3().load(\"../data\", group=\"Monthly\")[0]  #\n",
    "# df.sort_values([\"unique_id\", \"ds\"], inplace=True)\n",
    "\n",
    "# horizon = 12  # <-- FORECAST HORIZON\n",
    "# input_size = horizon * 5\n",
    "\n",
    "# batch_size = 32\n",
    "# step_size = 3\n",
    "\n",
    "import os\n",
    "\n",
    "# from ts.preprocess.bigdata.dataloader import TSPreprocessor, UnivariateTSDataModule\n",
    "\n",
    "preprocessor = TSPreprocessor(\n",
    "    df=df,\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    target_col=\"y_scaled\",\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    split_type=\"vertical\",\n",
    "    step_size=1,\n",
    "    cache_dir=\".\",\n",
    "    use_cache=True,\n",
    "    experiment_name=\"m5_experiment\",\n",
    "    num_workers=os.cpu_count() - 2,\n",
    "    chunk_size=1000,\n",
    "    adaptive_step=False,\n",
    "    downsample_factor=1,\n",
    "    save_by_window_count=True,\n",
    "    max_windows_per_file=1000,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "ds = UnivariateTSDataModule(\n",
    "    preprocessor=preprocessor,\n",
    "    batch_size=32,\n",
    "    num_workers=os.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=1,\n",
    "    persistent_workers=True,\n",
    "    gpu_preload=False,\n",
    "    experiment_name=\"m5_experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d93714-b1a1-43df-a23c-359d226c093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing UnivariateTSDataModule\n",
      "INFO:__main__:Setting up datamodule for stage: fit\n",
      "INFO:__main__:Initializing UnivariateTSDataset\n",
      "INFO:__main__:Initializing UnivariateTSDataset\n",
      "INFO:__main__:Getting train dataloader\n",
      "INFO:__main__:Creating dataloader\n",
      "INFO:__main__:Batch 1/5, time=7.07s, RAM delta=0.85GB, x.shape=torch.Size([32, 900]), y.shape=torch.Size([32, 180])\n",
      "INFO:__main__:Batch 2/5, time=13.16s, RAM delta=0.84GB, x.shape=torch.Size([32, 900]), y.shape=torch.Size([32, 180])\n",
      "INFO:__main__:Batch 3/5, time=19.26s, RAM delta=0.81GB, x.shape=torch.Size([32, 900]), y.shape=torch.Size([32, 180])\n",
      "INFO:__main__:Batch 4/5, time=25.37s, RAM delta=0.83GB, x.shape=torch.Size([32, 900]), y.shape=torch.Size([32, 180])\n",
      "INFO:__main__:Batch 5/5, time=31.66s, RAM delta=0.82GB, x.shape=torch.Size([32, 900]), y.shape=torch.Size([32, 180])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 427 ms, total: 1.73 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "ds = UnivariateTSDataModule(\n",
    "    preprocessor=preprocessor,\n",
    "    batch_size=32,  # Reduced\n",
    "    num_workers=1,  # Reduced\n",
    "    pin_memory=True,  # Disabled\n",
    "    prefetch_factor=1,\n",
    "    persistent_workers=False,  # Disabled\n",
    "    gpu_preload=False,\n",
    "    experiment_name=\"m5_experiment\",\n",
    ")\n",
    "\n",
    "# Profile DataLoader\n",
    "start_time = time.time()\n",
    "mem_before = psutil.virtual_memory().used / 1024**3  # GB\n",
    "ds.setup(\"fit\")\n",
    "train_loader = ds.train_dataloader()\n",
    "\n",
    "# Iterate over a few batches to profile\n",
    "num_batches = 5\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    mem_after = psutil.virtual_memory().used / 1024**3  # GB\n",
    "\n",
    "    logger.info(\n",
    "        f\"Batch {i+1}/{num_batches}, time={time.time() - start_time:.2f}s, \"\n",
    "        f\"RAM delta={mem_after - mem_before:.2f}GB, \"\n",
    "        f\"x.shape={x.shape}, y.shape={y.shape}\"\n",
    "    )\n",
    "    if i + 1 >= num_batches:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da196dc6-38a9-4a01-82e1-3c0a49c12428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ts.preprocess.bigdata.dataloader:Setting up datamodule for stage: fit\n",
      "INFO:ts.preprocess.bigdata.dataloader:Initializing UnivariateTSDataset\n",
      "INFO:ts.preprocess.bigdata.dataloader:Initializing UnivariateTSDataset\n",
      "INFO:ts.preprocess.bigdata.dataloader:Getting train dataloader\n",
      "INFO:ts.preprocess.bigdata.dataloader:Creating dataloader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 900]) torch.Size([32, 180])\n",
      "CPU times: user 27.3 s, sys: 5.67 s, total: 33 s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.setup(\"fit\")\n",
    "train_loader = ds.train_dataloader()\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0841ebd1-0b97-4390-846d-534f4fbcf479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ts.preprocess.bigdata.dataloader:Initializing UnivariateTSDataModule\n",
      "INFO:ts.preprocess.bigdata.dataloader:Setting up datamodule for stage: fit\n",
      "INFO:ts.preprocess.bigdata.dataloader:Initializing UnivariateTSDataset\n",
      "INFO:ts.preprocess.bigdata.dataloader:Initializing UnivariateTSDataset\n",
      "INFO:ts.preprocess.bigdata.dataloader:Getting train dataloader\n",
      "INFO:ts.preprocess.bigdata.dataloader:Creating dataloader\n",
      "Exception ignored in: <function UnivariateTSDataset.__del__ at 0x77930a75f6a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pranav-pc/projects/ts/ts/preprocess/bigdata/dataloader.py\", line 607, in __del__\n",
      "    h5_file.close()\n",
      "  File \"/home/pranav-pc/projects/ts/.venv/lib/python3.12/site-packages/h5py/_hl/files.py\", line 632, in close\n",
      "    self.id.close()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 900]) torch.Size([32, 180])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc93ba-3e4a-4d14-b99a-f08c08d8a342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703b892-839e-4317-9426-560d7f6b9bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6432661-1b48-4339-b0c5-7abebe6f12e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a639ef-ad39-48af-a57d-c9b74d7bfc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e00a93-ad56-4836-b2fc-ea69a83b626c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87f74-0420-4c97-b61c-6049b278af61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c776c6-5e62-47a4-856a-3dd226f76923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43d7a0-8add-4c96-8293-5b84319f1272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23a39b-989c-4e2a-a975-c87fc20cca5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21147a9-a9a0-4fdc-9c0b-c81aace8ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02195741-2347-49a7-b87d-8a047c28c814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae63370-9d6e-47c9-92bb-45bfbfe3f41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92332cc-da55-427d-a287-e2ffa8c29edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927d929-977f-412a-8393-d366818ded65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f2d90-67aa-488f-9005-bc55343ef0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ab85e-55ed-4bd2-a98f-2e145fe9f162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df73991-aead-440e-a745-46d00878eb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "780d2ada-ce18-40ee-b8f0-aa3cffd261f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_log_hyperparams': True,\n",
       " 'prepare_data_per_node': True,\n",
       " 'allow_zero_length_dataloader_with_multiple_devices': False,\n",
       " 'trainer': None,\n",
       " '_hparams_name': 'kwargs',\n",
       " '_hparams': \"batch_size\":         64\n",
       " \"experiment_name\":    m5_experiment\n",
       " \"gpu_preload\":        False\n",
       " \"num_workers\":        30\n",
       " \"persistent_workers\": True\n",
       " \"pin_memory\":         True\n",
       " \"prefetch_factor\":    2,\n",
       " '_hparams_initial': \"batch_size\":         64\n",
       " \"experiment_name\":    m5_experiment\n",
       " \"gpu_preload\":        False\n",
       " \"num_workers\":        30\n",
       " \"persistent_workers\": True\n",
       " \"pin_memory\":         True\n",
       " \"prefetch_factor\":    2,\n",
       " 'preprocessor': <ts.preprocess.bigdata.dataloader.TSPreprocessor at 0x7ff6a24db470>,\n",
       " 'batch_size': 64,\n",
       " 'num_workers': 24,\n",
       " 'pin_memory': True,\n",
       " 'prefetch_factor': 2,\n",
       " 'persistent_workers': True,\n",
       " 'gpu_preload': False,\n",
       " 'device': device(type='cpu'),\n",
       " 'experiment_name': 'm5_experiment',\n",
       " 'train_dataset': <ts.preprocess.bigdata.dataloader.UnivariateTSDataset at 0x7ff625d71160>,\n",
       " 'val_dataset': <ts.preprocess.bigdata.dataloader.UnivariateTSDataset at 0x7ff625d71340>,\n",
       " 'test_dataset': <ts.preprocess.bigdata.dataloader.UnivariateTSDataset at 0x7ff6a0223d70>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4dc2e3-1fa5-4385-abda-0a588f4d117b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/ts/preprocess/bigdata/dataloader.py:592\u001b[39m, in \u001b[36mUnivariateTSDataModule.train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    591\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mGetting train dataloader\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/ts/preprocess/bigdata/dataloader.py:579\u001b[39m, in \u001b[36mUnivariateTSDataModule._create_dataloader\u001b[39m\u001b[34m(self, dataset, shuffle)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    578\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCreating dataloader\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersistent_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:383\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    385\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:165\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.setup()\n",
    "for x, y in ds.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780e591-5fff-4618-a255-67e70a4cf52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
