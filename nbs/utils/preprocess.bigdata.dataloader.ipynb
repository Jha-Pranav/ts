{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe8adef-d861-4971-9d34-5bed2e376897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp preprocess.bigdata.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc73e725-bd0c-49b0-8a3f-6b6e64119b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import gc\n",
    "\n",
    "# from torch.serialization import safe_globals\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb915ed2-dc81-4b68-bde7-8e45445be373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.ERROR)  # Change to DEBUG for more details\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58f938c-14c5-4107-bf19-972d879d3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/projects/ts/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/pranav-pc/projects/ts/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:936: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5763cf-c538-4cac-ac44-fee79c6e0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import ray\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class TSPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        target_col=\"y_scaled\",\n",
    "        train_split=0.7,\n",
    "        val_split=0.15,\n",
    "        split_type=\"horizontal\",\n",
    "        step_size=1,\n",
    "        cache_dir=\".\",\n",
    "        use_cache=True,\n",
    "        experiment_name=\"default_experiment\",\n",
    "        num_workers=4,\n",
    "        chunk_size=1000,\n",
    "        adaptive_step=False,\n",
    "        downsample_factor=1,\n",
    "        large_dataset=True,\n",
    "        save_by_window_count=False,\n",
    "        max_windows_per_file=2000,\n",
    "        batch_size=200,\n",
    "    ):\n",
    "        logger.info(\"Initializing TSPreprocessor\")\n",
    "        self.df = df\n",
    "        self.target_col = target_col if target_col in df.columns else \"y\"\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.split_type = split_type\n",
    "        self.step_size = step_size\n",
    "        self.cache_dir = Path(cache_dir) / experiment_name\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.use_cache = use_cache\n",
    "        self.num_workers = min(num_workers, os.cpu_count() or 4)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.adaptive_step = adaptive_step\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.large_dataset = large_dataset\n",
    "        self.save_by_window_count = save_by_window_count\n",
    "        self.max_windows_per_file = max_windows_per_file\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if not 0 < train_split + val_split <= 1:\n",
    "            raise ValueError(\"train_split + val_split must be between 0 and 1\")\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must be >= 1\")\n",
    "        if downsample_factor < 1:\n",
    "            raise ValueError(\"downsample_factor must be >= 1\")\n",
    "        if max_windows_per_file < 1:\n",
    "            raise ValueError(\"max_windows_per_file must be >= 1\")\n",
    "        if batch_size < 1:\n",
    "            raise ValueError(\"batch_size must be >= 1\")\n",
    "\n",
    "        # Initialize Ray\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(num_cpus=self.num_workers, ignore_reinit_error=True)\n",
    "\n",
    "        self.train_windows, self.val_windows, self.test_windows = self._process_data()\n",
    "\n",
    "        del self.df\n",
    "        gc.collect()\n",
    "\n",
    "    def _generate_windows_in_memory(self, series):\n",
    "        series_len = len(series)\n",
    "        if series_len < self.input_size + self.horizon:\n",
    "            return []\n",
    "\n",
    "        max_idx = series_len - self.input_size - self.horizon + 1\n",
    "        if max_idx <= 0:\n",
    "            return []\n",
    "\n",
    "        window_starts = np.arange(0, max_idx, self.step_size, dtype=np.int32)\n",
    "        window_ends = window_starts + self.input_size\n",
    "        horizon_ends = window_ends + self.horizon\n",
    "\n",
    "        valid_windows = horizon_ends <= series_len\n",
    "        window_starts = window_starts[valid_windows]\n",
    "        window_ends = window_ends[valid_windows]\n",
    "        horizon_ends = horizon_ends[valid_windows]\n",
    "\n",
    "        x_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=self.input_size)[\n",
    "            window_starts\n",
    "        ]\n",
    "        y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])\n",
    "\n",
    "        return list(zip(x_windows, y_windows))\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_windows_optimized(\n",
    "        series,\n",
    "        unique_id,\n",
    "        split,\n",
    "        window_buffer=None,\n",
    "        chunk_size=None,\n",
    "        input_size=None,\n",
    "        horizon=None,\n",
    "        step_size=None,\n",
    "        adaptive_step=False,\n",
    "        cache_dir=None,\n",
    "    ):\n",
    "        series_len = len(series)\n",
    "        if series_len < input_size + horizon:\n",
    "            return window_buffer\n",
    "\n",
    "        max_idx = series_len - input_size - horizon + 1\n",
    "        if max_idx <= 0:\n",
    "            return window_buffer\n",
    "\n",
    "        step_size = step_size\n",
    "        if adaptive_step:\n",
    "            step_size = max(1, series_len // 1000)\n",
    "\n",
    "        chunk_size = chunk_size or max(100, min(1000, series_len // 10))\n",
    "\n",
    "        if window_buffer is not None:\n",
    "            if not isinstance(window_buffer, list):\n",
    "                window_buffer = list(window_buffer)\n",
    "            for start in range(0, max_idx, chunk_size * step_size):\n",
    "                end = min(start + chunk_size * step_size, max_idx)\n",
    "                window_starts = np.arange(start, end, step_size, dtype=np.int32)\n",
    "                window_ends = window_starts + input_size\n",
    "                horizon_ends = window_ends + horizon\n",
    "\n",
    "                valid_windows = horizon_ends <= series_len\n",
    "                window_starts = window_starts[valid_windows]\n",
    "                window_ends = window_ends[valid_windows]\n",
    "                horizon_ends = horizon_ends[valid_windows]\n",
    "\n",
    "                x_windows = np.lib.stride_tricks.sliding_window_view(\n",
    "                    series, window_shape=input_size\n",
    "                )[window_starts]\n",
    "                y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])\n",
    "\n",
    "                window_buffer.extend(zip(x_windows, y_windows))\n",
    "\n",
    "                del x_windows, y_windows\n",
    "                gc.collect()\n",
    "\n",
    "            return window_buffer\n",
    "        else:\n",
    "            h5_file = Path(cache_dir) / f\"{unique_id}_{split}.h5\"\n",
    "            with h5py.File(h5_file, \"a\") as f:\n",
    "                x_dset = (\n",
    "                    f[\"x\"]\n",
    "                    if \"x\" in f\n",
    "                    else f.create_dataset(\n",
    "                        \"x\",\n",
    "                        shape=(0, input_size),\n",
    "                        maxshape=(None, input_size),\n",
    "                        dtype=np.float32,\n",
    "                        compression=\"lzf\",\n",
    "                    )\n",
    "                )\n",
    "                y_dset = (\n",
    "                    f[\"y\"]\n",
    "                    if \"y\" in f\n",
    "                    else f.create_dataset(\n",
    "                        \"y\",\n",
    "                        shape=(0, horizon),\n",
    "                        maxshape=(None, horizon),\n",
    "                        dtype=np.float32,\n",
    "                        compression=\"lzf\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for start in range(0, max_idx, chunk_size * step_size):\n",
    "                    end = min(start + chunk_size * step_size, max_idx)\n",
    "                    window_starts = np.arange(start, end, step_size, dtype=np.int32)\n",
    "                    window_ends = window_starts + input_size\n",
    "                    horizon_ends = window_ends + horizon\n",
    "\n",
    "                    valid_windows = horizon_ends <= series_len\n",
    "                    window_starts = window_starts[valid_windows]\n",
    "                    window_ends = window_ends[valid_windows]\n",
    "                    horizon_ends = horizon_ends[valid_windows]\n",
    "\n",
    "                    x_windows = np.lib.stride_tricks.sliding_window_view(\n",
    "                        series, window_shape=input_size\n",
    "                    )[window_starts]\n",
    "                    y_windows = np.stack(\n",
    "                        [series[we:he] for we, he in zip(window_ends, horizon_ends)]\n",
    "                    )\n",
    "\n",
    "                    x_dset.resize(x_dset.shape[0] + len(x_windows), axis=0)\n",
    "                    y_dset.resize(y_dset.shape[0] + len(y_windows), axis=0)\n",
    "                    x_dset[-len(x_windows) :] = x_windows\n",
    "                    y_dset[-len(y_windows) :] = y_windows\n",
    "\n",
    "                    del x_windows, y_windows\n",
    "                    gc.collect()\n",
    "\n",
    "    @staticmethod\n",
    "    @ray.remote\n",
    "    def _save_buffered_windows(window_buffer, file_idx, split, cache_dir, input_size, horizon):\n",
    "        if not window_buffer:\n",
    "            return file_idx\n",
    "\n",
    "        h5_file = Path(cache_dir) / f\"windows_{file_idx}_{split}.h5\"\n",
    "        with h5py.File(h5_file, \"a\") as f:\n",
    "            x_dset = (\n",
    "                f[\"x\"]\n",
    "                if \"x\" in f\n",
    "                else f.create_dataset(\n",
    "                    \"x\",\n",
    "                    shape=(0, input_size),\n",
    "                    maxshape=(None, input_size),\n",
    "                    dtype=np.float32,\n",
    "                    compression=\"lzf\",\n",
    "                )\n",
    "            )\n",
    "            y_dset = (\n",
    "                f[\"y\"]\n",
    "                if \"y\" in f\n",
    "                else f.create_dataset(\n",
    "                    \"y\",\n",
    "                    shape=(0, horizon),\n",
    "                    maxshape=(None, horizon),\n",
    "                    dtype=np.float32,\n",
    "                    compression=\"lzf\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            x_windows, y_windows = zip(*window_buffer)\n",
    "            x_windows = np.stack(x_windows)\n",
    "            y_windows = np.stack(y_windows)\n",
    "\n",
    "            x_dset.resize(x_dset.shape[0] + len(x_windows), axis=0)\n",
    "            y_dset.resize(y_dset.shape[0] + len(y_windows), axis=0)\n",
    "            x_dset[-len(x_windows) :] = x_windows\n",
    "            y_dset[-len(y_windows) :] = y_windows\n",
    "\n",
    "        del window_buffer[:]\n",
    "        gc.collect()\n",
    "        return file_idx + 1\n",
    "\n",
    "    def _process_one_series_in_memory(self, unique_id_group):\n",
    "        unique_id, group = unique_id_group\n",
    "        series = group[self.target_col].values.astype(np.float32)\n",
    "\n",
    "        windows = self._generate_windows_in_memory(series)\n",
    "        del series\n",
    "        gc.collect()\n",
    "\n",
    "        if not windows:\n",
    "            logger.warning(f\"[{unique_id}] Series too short to generate any windows\")\n",
    "            return [], [], [], unique_id\n",
    "\n",
    "        if self.split_type == \"horizontal\":\n",
    "            num_windows = len(windows)\n",
    "            train_end = int(num_windows * self.train_split)\n",
    "            val_end = train_end + int(num_windows * self.val_split)\n",
    "            return (windows[:train_end], windows[train_end:val_end], windows[val_end:], unique_id)\n",
    "        else:\n",
    "            return windows, [], [], unique_id\n",
    "\n",
    "    @staticmethod\n",
    "    @ray.remote(num_cpus=1)  # Allocate 1 CPU per task for full utilization\n",
    "    def _process_batch_optimized(\n",
    "        unique_id_groups,\n",
    "        window_buffers,\n",
    "        file_indices,\n",
    "        cache_dir,\n",
    "        input_size,\n",
    "        horizon,\n",
    "        step_size,\n",
    "        adaptive_step,\n",
    "        downsample_factor,\n",
    "        chunk_size,\n",
    "        max_windows_per_file,\n",
    "        split_type,\n",
    "        train_split,\n",
    "        val_split,\n",
    "        target_col,\n",
    "    ):\n",
    "        results = []\n",
    "        for unique_id_group in unique_id_groups:\n",
    "            unique_id, group = unique_id_group\n",
    "            series = group[target_col].values.astype(np.float32)\n",
    "\n",
    "            if len(series) < input_size + horizon:\n",
    "                continue  # Skip short series\n",
    "\n",
    "            if downsample_factor > 1:\n",
    "                series = series[::downsample_factor]\n",
    "\n",
    "            step_size = step_size\n",
    "            if adaptive_step:\n",
    "                step_size = max(1, len(series) // 1000)\n",
    "\n",
    "            if split_type == \"horizontal\":\n",
    "                series_len = len(series)\n",
    "                num_windows = max(0, (series_len - input_size - horizon + 1) // step_size)\n",
    "                train_end = int(num_windows * train_split)\n",
    "                val_end = train_end + int(num_windows * val_split)\n",
    "\n",
    "                result = []\n",
    "                if window_buffers is not None:\n",
    "                    for split, start, end in [\n",
    "                        (\"train\", 0, train_end * step_size),\n",
    "                        (\"val\", train_end * step_size, val_end * step_size),\n",
    "                        (\"test\", val_end * step_size, series_len),\n",
    "                    ]:\n",
    "                        buffer = TSPreprocessor._generate_windows_optimized(\n",
    "                            series[start:end],\n",
    "                            unique_id,\n",
    "                            split,\n",
    "                            window_buffers.get(split),\n",
    "                            chunk_size,\n",
    "                            input_size,\n",
    "                            horizon,\n",
    "                            step_size,\n",
    "                            adaptive_step,\n",
    "                            cache_dir,\n",
    "                        )\n",
    "                        if buffer:\n",
    "                            window_buffers[split] = buffer\n",
    "                        result.append((None, unique_id, split))\n",
    "                    results.append((result, window_buffers, file_indices))\n",
    "                else:\n",
    "                    for split, start, end in [\n",
    "                        (\"train\", 0, train_end * step_size),\n",
    "                        (\"val\", train_end * step_size, val_end * step_size),\n",
    "                        (\"test\", val_end * step_size, series_len),\n",
    "                    ]:\n",
    "                        TSPreprocessor._generate_windows_optimized(\n",
    "                            series[start:end],\n",
    "                            unique_id,\n",
    "                            split,\n",
    "                            None,\n",
    "                            chunk_size,\n",
    "                            input_size,\n",
    "                            horizon,\n",
    "                            step_size,\n",
    "                            adaptive_step,\n",
    "                            cache_dir,\n",
    "                        )\n",
    "                        result.append(\n",
    "                            (Path(cache_dir) / f\"{unique_id}_{split}.h5\", unique_id, split)\n",
    "                        )\n",
    "                    results.append((result, window_buffers, file_indices))\n",
    "            else:\n",
    "                if window_buffers is not None:\n",
    "                    buffer = TSPreprocessor._generate_windows_optimized(\n",
    "                        series,\n",
    "                        unique_id,\n",
    "                        \"all\",\n",
    "                        window_buffers.get(\"all\"),\n",
    "                        chunk_size,\n",
    "                        input_size,\n",
    "                        horizon,\n",
    "                        step_size,\n",
    "                        adaptive_step,\n",
    "                        cache_dir,\n",
    "                    )\n",
    "                    if buffer:\n",
    "                        window_buffers[\"all\"] = buffer\n",
    "                    results.append(([(None, unique_id, \"all\")], window_buffers, file_indices))\n",
    "                else:\n",
    "                    TSPreprocessor._generate_windows_optimized(\n",
    "                        series,\n",
    "                        unique_id,\n",
    "                        \"all\",\n",
    "                        None,\n",
    "                        chunk_size,\n",
    "                        input_size,\n",
    "                        horizon,\n",
    "                        step_size,\n",
    "                        adaptive_step,\n",
    "                        cache_dir,\n",
    "                    )\n",
    "                    results.append(\n",
    "                        (\n",
    "                            [(Path(cache_dir) / f\"{unique_id}_all.h5\", unique_id, \"all\")],\n",
    "                            window_buffers,\n",
    "                            file_indices,\n",
    "                        )\n",
    "                    )\n",
    "        return results\n",
    "\n",
    "    def _process_data(self):\n",
    "        logger.info(\"Processing data for all unique_ids\")\n",
    "        cache_file = self.cache_dir / \"preprocessed_windows.pkl\"\n",
    "\n",
    "        if self.use_cache and cache_file.exists():\n",
    "            logger.info(\"Loading preprocessed windows from cache\")\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            return data[\"train_windows\"], data[\"val_windows\"], data[\"test_windows\"]\n",
    "\n",
    "        grouped = list(self.df.groupby(\"unique_id\"))\n",
    "        train_windows, val_windows, test_windows = [], [], []\n",
    "\n",
    "        if self.split_type == \"vertical\":\n",
    "            logger.info(\"Applying vertical split\")\n",
    "            unique_ids = list(self.df[\"unique_id\"].unique())\n",
    "            np.random.shuffle(unique_ids)\n",
    "            total_series = len(unique_ids)\n",
    "            train_end = int(total_series * self.train_split)\n",
    "            val_end = train_end + int(total_series * self.val_split)\n",
    "\n",
    "            train_ids = unique_ids[:train_end]\n",
    "            val_ids = unique_ids[train_end:val_end]\n",
    "            test_ids = unique_ids[val_end:]\n",
    "\n",
    "        if self.large_dataset:\n",
    "            window_buffers = {\"train\": [], \"val\": [], \"test\": [], \"all\": []}\n",
    "            file_indices = {\"train\": 0, \"val\": 0, \"test\": 0, \"all\": 0}\n",
    "\n",
    "            # Batch unique IDs to reduce task overhead\n",
    "            batches = [\n",
    "                grouped[i : i + self.batch_size] for i in range(0, len(grouped), self.batch_size)\n",
    "            ]\n",
    "\n",
    "            # Submit Ray tasks for processing batches\n",
    "            futures = [\n",
    "                TSPreprocessor._process_batch_optimized.remote(\n",
    "                    batch,\n",
    "                    window_buffers,\n",
    "                    file_indices,\n",
    "                    self.cache_dir,\n",
    "                    self.input_size,\n",
    "                    self.horizon,\n",
    "                    self.step_size,\n",
    "                    self.adaptive_step,\n",
    "                    self.downsample_factor,\n",
    "                    self.chunk_size,\n",
    "                    self.max_windows_per_file,\n",
    "                    self.split_type,\n",
    "                    self.train_split,\n",
    "                    self.val_split,\n",
    "                    self.target_col,\n",
    "                )\n",
    "                for batch in batches\n",
    "            ]\n",
    "\n",
    "            # Collect results with tqdm progress bar\n",
    "            pbar = tqdm(total=len(futures), desc=\"Processing batches\", dynamic_ncols=True)\n",
    "            results = []\n",
    "            while futures:\n",
    "                done, futures = ray.wait(futures, num_returns=min(len(futures), 1))\n",
    "                batch_results = ray.get(done)[0]  # Unpack batch results\n",
    "                results.extend(batch_results)\n",
    "                pbar.update(len(done))\n",
    "            pbar.close()\n",
    "\n",
    "            if self.save_by_window_count:\n",
    "                futures = []\n",
    "                for split in window_buffers:\n",
    "                    buffer = window_buffers[split]\n",
    "                    file_idx = file_indices[split]\n",
    "                    while buffer:\n",
    "                        futures.append(\n",
    "                            TSPreprocessor._save_buffered_windows.remote(\n",
    "                                buffer[: self.max_windows_per_file],\n",
    "                                file_idx,\n",
    "                                split,\n",
    "                                self.cache_dir,\n",
    "                                self.input_size,\n",
    "                                self.horizon,\n",
    "                            )\n",
    "                        )\n",
    "                        train_windows.append(\n",
    "                            (self.cache_dir / f\"windows_{file_idx}_{split}.h5\", None)\n",
    "                        )\n",
    "                        buffer = buffer[self.max_windows_per_file :]\n",
    "                        file_idx += 1\n",
    "                    file_indices[split] = file_idx\n",
    "\n",
    "                # Save windows with tqdm progress bar\n",
    "                pbar = tqdm(total=len(futures), desc=\"Saving windows\", dynamic_ncols=True)\n",
    "                while futures:\n",
    "                    done, futures = ray.wait(futures, num_returns=min(len(futures), 1))\n",
    "                    for file_idx in ray.get(done):\n",
    "                        file_indices[split] = max(file_indices[split], file_idx)\n",
    "                    pbar.update(len(done))\n",
    "                pbar.close()\n",
    "\n",
    "            for result, _, _ in results:\n",
    "                for h5_file, unique_id, split in result:\n",
    "                    if h5_file:\n",
    "                        if self.split_type == \"horizontal\":\n",
    "                            if split == \"train\":\n",
    "                                train_windows.append((h5_file, unique_id))\n",
    "                            elif split == \"val\":\n",
    "                                val_windows.append((h5_file, unique_id))\n",
    "                            elif split == \"test\":\n",
    "                                test_windows.append((h5_file, unique_id))\n",
    "                        else:\n",
    "                            if unique_id in train_ids:\n",
    "                                train_windows.append((h5_file, unique_id))\n",
    "                            elif unique_id in val_ids:\n",
    "                                val_windows.append((h5_file, unique_id))\n",
    "                            elif unique_id in test_ids:\n",
    "                                test_windows.append((h5_file, unique_id))\n",
    "        else:\n",
    "            from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                results = list(\n",
    "                    tqdm(\n",
    "                        executor.map(self._process_one_series_in_memory, grouped),\n",
    "                        total=len(grouped),\n",
    "                        desc=\"Processing series\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            for train_w, val_w, test_w, unique_id in results:\n",
    "                if self.split_type == \"horizontal\":\n",
    "                    train_windows.extend(train_w)\n",
    "                    val_windows.extend(val_w)\n",
    "                    test_windows.extend(test_w)\n",
    "                else:\n",
    "                    if unique_id in train_ids:\n",
    "                        train_windows.extend(train_w)\n",
    "                    elif unique_id in val_ids:\n",
    "                        val_windows.extend(train_w)\n",
    "                    elif unique_id in test_ids:\n",
    "                        test_windows.extend(train_w)\n",
    "\n",
    "        train_windows = np.array(train_windows, dtype=object)\n",
    "        val_windows = np.array(val_windows, dtype=object)\n",
    "        test_windows = np.array(test_windows, dtype=object)\n",
    "\n",
    "        if self.use_cache:\n",
    "            logger.info(\"Saving preprocessed windows metadata to cache\")\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"train_windows\": train_windows,\n",
    "                        \"val_windows\": val_windows,\n",
    "                        \"test_windows\": test_windows,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Train windows: {len(train_windows)}, Val windows: {len(val_windows)}, Test windows: {len(test_windows)}\"\n",
    "        )\n",
    "        return train_windows, val_windows, test_windows\n",
    "\n",
    "\n",
    "class UnivariateTSDataset(Dataset):\n",
    "    def __init__(self, windows, device: str = None, large_dataset: bool = True):\n",
    "        logger.info(\"Initializing UnivariateTSDataset\")\n",
    "        self.windows = windows\n",
    "        self.device = device\n",
    "        self.large_dataset = large_dataset\n",
    "        if large_dataset:\n",
    "            self.h5_files = {}\n",
    "        else:\n",
    "            self.x = np.stack([w[0] for w in windows], axis=0).astype(np.float32)\n",
    "            self.y = np.stack([w[1] for w in windows], axis=0).astype(np.float32)\n",
    "            if device:\n",
    "                logger.info(\"Preloading tensors to device: %s\", device)\n",
    "                self.x_tensor = torch.from_numpy(self.x).to(device)\n",
    "                self.y_tensor = torch.from_numpy(self.y).to(device)\n",
    "            else:\n",
    "                self.x_tensor = self.y_tensor = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.large_dataset:\n",
    "            total_len = 0\n",
    "            for h5_file, _ in self.windows:\n",
    "                if h5_file not in self.h5_files:\n",
    "                    self.h5_files[h5_file] = h5py.File(h5_file, \"r\")\n",
    "                total_len += self.h5_files[h5_file][\"x\"].shape[0]\n",
    "            return total_len\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.large_dataset:\n",
    "            current_idx = 0\n",
    "            for h5_file, _ in self.windows:\n",
    "                if h5_file not in self.h5_files:\n",
    "                    self.h5_files[h5_file] = h5py.File(h5_file, \"r\")\n",
    "                num_windows = self.h5_files[h5_file][\"x\"].shape[0]\n",
    "                if current_idx + num_windows > idx:\n",
    "                    local_idx = idx - current_idx\n",
    "                    x = self.h5_files[h5_file][\"x\"][local_idx]\n",
    "                    y = self.h5_files[h5_file][\"y\"][local_idx]\n",
    "                    if self.device:\n",
    "                        return torch.from_numpy(x).to(self.device), torch.from_numpy(y).to(\n",
    "                            self.device\n",
    "                        )\n",
    "                    return torch.from_numpy(x), torch.from_numpy(y)\n",
    "                current_idx += num_windows\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        else:\n",
    "            if self.device:\n",
    "                return self.x_tensor[idx], self.y_tensor[idx]\n",
    "            return torch.from_numpy(self.x[idx]), torch.from_numpy(self.y[idx])\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.large_dataset:\n",
    "            for h5_file in self.h5_files.values():\n",
    "                h5_file.close()\n",
    "\n",
    "\n",
    "class UnivariateTSDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor: TSPreprocessor,\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True,\n",
    "        gpu_preload=False,\n",
    "        experiment_name=\"default_experiment\",\n",
    "    ):\n",
    "        logger.info(\"Initializing UnivariateTSDataModule\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"preprocessor\"])\n",
    "        self.preprocessor = preprocessor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = min(num_workers, torch.get_num_threads())\n",
    "        self.pin_memory = pin_memory and not gpu_preload\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.gpu_preload = gpu_preload\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        logger.info(f\"Setting up datamodule for stage: {stage}\")\n",
    "        if stage in (\"fit\", None):\n",
    "            self.train_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.train_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "            self.val_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.val_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "        if stage in (\"validate\", None):\n",
    "            self.val_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.val_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "        if stage in (\"test\", None):\n",
    "            self.test_dataset = UnivariateTSDataset(\n",
    "                self.preprocessor.test_windows,\n",
    "                device=self.device if self.gpu_preload else None,\n",
    "                large_dataset=self.preprocessor.large_dataset,\n",
    "            )\n",
    "\n",
    "    def _create_dataloader(self, dataset, shuffle=False):\n",
    "        logger.info(\"Creating dataloader\")\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            prefetch_factor=self.prefetch_factor,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            drop_last=shuffle,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        logger.info(\"Getting train dataloader\")\n",
    "        return self._create_dataloader(self.train_dataset, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        logger.info(\"Getting val dataloader\")\n",
    "        return self._create_dataloader(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        logger.info(\"Getting test dataloader\")\n",
    "        return self._create_dataloader(self.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f802e1a-b6c4-434f-a874-dab5db19cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 16:24:15,272\tINFO worker.py:1852 -- Started a local Ray instance.\n",
      "/tmp/ipykernel_205704/1914704754.py:304: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped = list(self.df.groupby(\"unique_id\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23928428adb14f5c85ff69c586d9f3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|                                                                                     â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 6059 MiB, 137 objects, write throughput 777 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 6553 MiB, 144 objects, write throughput 706 MiB/s.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-04 16:25:15,257 E 206186 206186] (raylet) node_manager.cc:3313: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 90f2778e5190063d902fb7fd1d2a566fc5585bc4c0e4efecee09a2b5, IP: 192.168.0.112) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.0.112`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 9153 MiB, 153 objects, write throughput 797 MiB/s.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 16738 MiB, 189 objects, write throughput 688 MiB/s.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example usage\n",
    "## Data loading and preprocessing df.columns - unique_id,ds,y\n",
    "from datasetsforecast.m5 import M5\n",
    "\n",
    "df = M5().load(\"../data\")[0]  # , group=\"Monthly\"\n",
    "df.sort_values([\"unique_id\", \"ds\"], inplace=True)\n",
    "\n",
    "\n",
    "horizon = 180  # <-- FORECAST HORIZON\n",
    "input_size = horizon * 6\n",
    "\n",
    "# from ts.preprocess.dataloader import TSPreprocessor, UnivariateTSDataModule\n",
    "import os\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = os.cpu_count() - 6\n",
    "step_size = 1\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TSPreprocessor(\n",
    "    df=df,\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    target_col=\"y_scaled\",  # Default to y_scaled, falls back to y\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    split_type=\"vertical\",\n",
    "    step_size=step_size,\n",
    "    cache_dir=\".\",\n",
    "    use_cache=True,\n",
    "    experiment_name=\"m5_experiment\",\n",
    "    num_workers=num_workers,\n",
    "    chunk_size=1000,\n",
    "    adaptive_step=False,\n",
    "    downsample_factor=1,\n",
    "    large_dataset=True,  # Set to False for original in-memory approach\n",
    "    save_by_window_count=True,  # Set to True to save by window count\n",
    "    max_windows_per_file=5000,  # Number of windows per HDF5 file\n",
    ")\n",
    "\n",
    "# Initialize DataModule\n",
    "ds = UnivariateTSDataModule(\n",
    "    preprocessor=preprocessor,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    "    gpu_preload=False,\n",
    "    experiment_name=\"m5_experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4dc2e3-1fa5-4385-abda-0a588f4d117b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.setup()\n",
    "for x, y in ds.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bdd71-f8b9-4457-97d6-31adb1be84bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2223092-b084-4318-a87a-2abb6d22d910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
