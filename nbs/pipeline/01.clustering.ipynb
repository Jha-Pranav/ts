{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0734356-9bf1-4173-b185-2bd0039d3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/intermediate/m5_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7b07e-580a-41dc-9161-217dfe637b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "\n",
    "db = DBSCAN(eps=0.5, min_samples=5, metric=\"euclidean\")\n",
    "db_labels = db.fit_predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70595be5-7a29-45c6-8821-ca36a736a37a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_scaled = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33my_scaled\u001b[39m\u001b[33m\"\u001b[39m].values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m agglomerative = AgglomerativeClustering(n_clusters=\u001b[38;5;28;01mNone\u001b[39;00m, distance_threshold=\u001b[32m0.5\u001b[39m)\n\u001b[32m      3\u001b[39m agglomerative_labels = agglomerative.fit_predict(df_scaled)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_scaled = df[\"y_scaled\"].values.reshape(-1, 1)\n",
    "agglomerative = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5)\n",
    "agglomerative_labels = agglomerative.fit_predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c182a6dc-7e48-4c6d-82ff-b1e297dbe4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/pranav-pc/.pyenv/versions/3.12.9/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/pranav-pc/.pyenv/versions/3.12.9/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/pranav-pc/.pyenv/versions/3.12.9/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/pranav-pc/.pyenv/versions/3.12.9/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/pranav-pc/.pyenv/versions/3.12.9/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_62277/3440801400.py\", line 11, in <module>\n",
      "    from sklearn_extra.cluster import KMedoids\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/__init__.py\", line 1, in <module>\n",
      "    from . import kernel_approximation, kernel_methods  # noqa\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/kernel_approximation/__init__.py\", line 1, in <module>\n",
      "    from ._fastfood import Fastfood\n",
      "  File \"/home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/kernel_approximation/_fastfood.py\", line 11, in <module>\n",
      "    from ..utils._cyfht import fht2 as cyfht\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtslearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KShape\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtslearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesScalerMeanVariance\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_extra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMedoids\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AffinityPropagation\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Define clustering functions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kernel_approximation, kernel_methods  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m      5\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/kernel_approximation/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fastfood\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fastfood\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mFastfood\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages/sklearn_extra/kernel_approximation/_fastfood.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cyfht\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fht2 \u001b[38;5;28;01mas\u001b[39;00m cyfht\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFastfood\u001b[39;00m(BaseEstimator, TransformerMixin):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Approximates feature map of an RBF kernel by Monte Carlo approximation\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    of its Fourier transform.\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn_extra/utils/_cyfht.pyx:1\u001b[39m, in \u001b[36minit sklearn_extra.utils._cyfht\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# Define clustering functions\n",
    "\n",
    "\n",
    "def perform_dbscan(df_scaled):\n",
    "    db = DBSCAN(eps=0.5, min_samples=5, metric=\"euclidean\")\n",
    "    db_labels = db.fit_predict(df_scaled)\n",
    "    return db_labels\n",
    "\n",
    "\n",
    "def perform_agglomerative_clustering(df_scaled):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5)\n",
    "    agglomerative_labels = agglomerative.fit_predict(df_scaled)\n",
    "    return agglomerative_labels\n",
    "\n",
    "\n",
    "# def perform_k_medoids(df_scaled):\n",
    "#     kmedoids = KMedoids(n_clusters=3, metric=\"euclidean\")\n",
    "#     kmedoids_labels = kmedoids.fit_predict(df_scaled)\n",
    "#     return kmedoids_labels\n",
    "\n",
    "\n",
    "def perform_gmm(df_scaled):\n",
    "    gmm = GaussianMixture(n_components=3)\n",
    "    gmm_labels = gmm.fit_predict(df_scaled)\n",
    "    return gmm_labels\n",
    "\n",
    "\n",
    "def perform_k_shape(df_scaled):\n",
    "    ts = TimeSeriesScalerMeanVariance()\n",
    "    df_scaled = ts.fit_transform(df_scaled)\n",
    "    kshape = KShape(n_clusters=3, verbose=True)\n",
    "    kshape_labels = kshape.fit_predict(df_scaled)\n",
    "    return kshape_labels\n",
    "\n",
    "\n",
    "def perform_affinity_propagation(df_scaled):\n",
    "    affprop = AffinityPropagation(damping=0.9)\n",
    "    affprop_labels = affprop.fit_predict(df_scaled)\n",
    "    return affprop_labels\n",
    "\n",
    "\n",
    "# Main function to cluster and return results\n",
    "\n",
    "\n",
    "def cluster_time_series(df_scaled, unique_id_col=\"unique_id\", y_scaled_col=\"y_scaled\"):\n",
    "\n",
    "    # Perform clustering using all methods\n",
    "    dbscan_labels = perform_dbscan(df_scaled)\n",
    "    agglomerative_labels = perform_agglomerative_clustering(df_scaled)\n",
    "    # kmedoids_labels = perform_k_medoids(df_scaled)\n",
    "    gmm_labels = perform_gmm(df_scaled)\n",
    "    kshape_labels = perform_k_shape(df_scaled)\n",
    "    affprop_labels = perform_affinity_propagation(df_scaled)\n",
    "\n",
    "    # Add results back to the dataframe\n",
    "    df[\"dbscan_labels\"] = dbscan_labels\n",
    "    df[\"agglomerative_labels\"] = agglomerative_labels\n",
    "    # df['kmedoids_labels'] = kmedoids_labels\n",
    "    df[\"gmm_labels\"] = gmm_labels\n",
    "    df[\"kshape_labels\"] = kshape_labels\n",
    "    df[\"affprop_labels\"] = affprop_labels\n",
    "\n",
    "    # Optionally compute silhouette scores for evaluation\n",
    "    silhouette_scores = {\n",
    "        \"DBSCAN\": silhouette_score(df_scaled, dbscan_labels),\n",
    "        \"Agglomerative\": silhouette_score(df_scaled, agglomerative_labels),\n",
    "        # 'K-Medoids': silhouette_score(df_scaled, kmedoids_labels),\n",
    "        \"GMM\": silhouette_score(df_scaled, gmm_labels),\n",
    "        \"K-Shape\": silhouette_score(df_scaled, kshape_labels),\n",
    "        \"Affinity Propagation\": silhouette_score(df_scaled, affprop_labels),\n",
    "    }\n",
    "\n",
    "    # Print the silhouette scores for evaluation\n",
    "    for method, score in silhouette_scores.items():\n",
    "        print(f\"Silhouette Score for {method}: {score:.4f}\")\n",
    "\n",
    "    return df, silhouette_scores\n",
    "\n",
    "\n",
    "# Function to visualize time series clusters using Plotly\n",
    "\n",
    "\n",
    "def plot_clusters(df, label_col, n_clusters=6):\n",
    "    # Filter random samples\n",
    "    unique_labels = np.unique(df[label_col])\n",
    "    sample_clusters = np.random.choice(unique_labels, size=n_clusters, replace=False)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for cluster in sample_clusters:\n",
    "        cluster_data = df[df[label_col] == cluster]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.arange(len(cluster_data)),\n",
    "                y=cluster_data[\"y_scaled\"],\n",
    "                mode=\"lines\",\n",
    "                name=f\"Cluster {cluster}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Random {n_clusters} Time Series from {label_col} Clusters\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Scaled Value\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage with a dummy DataFrame (replace with your actual data)\n",
    "df = pd.read_parquet(\"data/intermediate/m5_scaled.parquet\")  # Replace with your dataset\n",
    "df_with_labels, silhouette_scores = cluster_time_series(df)\n",
    "\n",
    "# Display the result with cluster labels\n",
    "print(df_with_labels)\n",
    "\n",
    "# Visualize the time series of each cluster\n",
    "for label_col in [\n",
    "    \"dbscan_labels\",\n",
    "    \"agglomerative_labels\",\n",
    "    \"kmedoids_labels\",\n",
    "    \"gmm_labels\",\n",
    "    \"kshape_labels\",\n",
    "    \"affprop_labels\",\n",
    "]:\n",
    "    print(f\"\\nVisualizing {label_col}\")\n",
    "    plot_clusters(df_with_labels, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce53da0-8661-420b-8795-3618a82e46fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Downloading scikit-learn-extra-0.3.0.tar.gz (818 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages (from scikit-learn-extra) (2.2.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages (from scikit-learn-extra) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages (from scikit-learn-extra) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/pranav-pc/projects/ts/nbs/clustering/lab/lib/python3.12/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.6.0)\n",
      "Building wheels for collected packages: scikit-learn-extra\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.3.0-cp312-cp312-linux_x86_64.whl size=2525905 sha256=dc8fd5bbf932df0ceefbcfcdb9ec33dee078ab14c209bf16a6d38c1a2260ba6c\n",
      "  Stored in directory: /home/pranav-pc/.cache/pip/wheels/17/4d/c3/c6d5d563c1bf8146d059d63be3678abc2f2801fba0aaf5f0b8\n",
      "Successfully built scikit-learn-extra\n",
      "Installing collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07704a-ea75-4157-a7e5-76f5b7ed6dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab)",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
