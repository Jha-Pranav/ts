{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc57ad-08cd-4f54-89f8-d7600e8f7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp classification.pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b18588-16ce-4e2d-94e1-661cc3224932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasetsforecast.m4 import M4\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7205fe51-7ce5-4a44-878c-f5af2812d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import warnings\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=InterpolationWarning)\n",
    "\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy.signal._spectral_py\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Suppress InterpolationWarning specifically\n",
    "warnings.filterwarnings(\"ignore\", category=InterpolationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10879275-2570-4e8f-a56d-ea6464118f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29138/3507858170.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  no_of_datapoints = m4_dataset.groupby(\"unique_id\").apply(len).to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing Complete! Data saved as Parquet.\n"
     ]
    }
   ],
   "source": [
    "## Prepare data\n",
    "\n",
    "# Load Labels\n",
    "label_df = pd.read_parquet(\"data/evaluation_df.parquet\").set_index(\"unique_id\")[\"best_model\"]\n",
    "\n",
    "# Load M4 dataset\n",
    "groups = [\"Yearly\", \"Monthly\", \"Quarterly\", \"Hourly\", \"Weekly\", \"Daily\"]\n",
    "m4_df_bucket = []\n",
    "for group in groups:\n",
    "    await M4.async_download(\"data\", group=group)\n",
    "    Y_df, *_ = M4.load(directory=\"data\", group=group)\n",
    "    m4_df_bucket.append(Y_df)\n",
    "m4_dataset = pd.concat(m4_df_bucket)\n",
    "\n",
    "m4_dataset = m4_dataset.sort_values([\"unique_id\", \"ds\"]).drop_duplicates(\n",
    "    subset=[\"unique_id\", \"ds\"]\n",
    ")\n",
    "no_of_datapoints = m4_dataset.groupby(\"unique_id\").apply(len).to_dict()\n",
    "\n",
    "# Convert to wide format (fixed)\n",
    "m4_dataset = m4_dataset.pivot(index=\"unique_id\", columns=\"ds\", values=\"y\")\n",
    "\n",
    "# # Merge with labels\n",
    "m4_dataset = m4_dataset.merge(label_df, left_index=True, right_index=True, how=\"right\")\n",
    "\n",
    "best_model = m4_dataset[\"best_model\"].to_dict()\n",
    "# df_min = m4_dataset.drop(\"best_model\", axis=1).min(axis=1)\n",
    "# df_max = m4_dataset.drop(\"best_model\", axis=1).max(axis=1)\n",
    "\n",
    "# m4_dataset = (m4_dataset.drop(\"best_model\", axis=1) - df_min.values.reshape(-1, 1)) / (\n",
    "#     df_max - df_min\n",
    "# ).values.reshape(-1, 1)\n",
    "\n",
    "m4_dataset[\"best_model\"] = m4_dataset.index.map(best_model)\n",
    "m4_dataset[\"no_of_datapoints\"] = m4_dataset.index.map(no_of_datapoints)\n",
    "\n",
    "m4_dataset.sort_values(\"no_of_datapoints\", inplace=True)\n",
    "# m4_dataset.drop(\"no_of_datapoints\", axis=1, inplace=True)\n",
    "# # clean up the memory\n",
    "# del df_max\n",
    "# del df_min\n",
    "del m4_df_bucket, no_of_datapoints\n",
    "\n",
    "\n",
    "m4_dataset.columns = m4_dataset.columns.astype(str)\n",
    "\n",
    "# # Save as optimized Parquet\n",
    "m4_dataset.to_parquet(\"data/m4_preprocessed.parquet\", engine=\"fastparquet\", compression=\"snappy\")\n",
    "\n",
    "print(\"✅ Preprocessing Complete! Data saved as Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51932d17-b716-4c1e-93d5-25e51d5dcf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42728fb09a804bf18c6b607cdafdcc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ts.commons.stats import extract_stats_features\n",
    "\n",
    "df = pd.read_parquet(\"data/m4_preprocessed.parquet\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "X = df.drop([\"no_of_datapoints\", \"best_model\"], axis=1)\n",
    "y = df[\"best_model\"]\n",
    "\n",
    "\n",
    "# Function to extract features for a single row\n",
    "def extract_features(idx, name):\n",
    "    features = extract_stats_features(X.iloc[idx].dropna(), max_lag=10)\n",
    "    return name, features\n",
    "\n",
    "\n",
    "# Sequential processing with progress bar\n",
    "results = [extract_features(idx, name) for idx, name in tqdm(enumerate(X.index), total=len(X))]\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_features = pd.DataFrame(dict(results)).T  # Transpose to get features as columns\n",
    "# Save to Parquet\n",
    "df_features.merge(\n",
    "    df[[\"best_model\", \"no_of_datapoints\"]], right_index=True, left_index=True, how=\"left\"\n",
    ").to_parquet(\"data/extracted_features.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "print(\"Feature extraction completed and saved to 'extracted_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc7497-206d-4a5b-a777-268f570f4215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
