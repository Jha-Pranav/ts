{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8b0db-1c55-48d4-91ac-e3f3129ed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp benchmark.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40770940-3395-4476-8a04-3cc70db4a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2063ba7a-37ea-4a57-802d-bb39a2d97444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TimeSeriesBenchmarkDataset:\n",
    "    def __init__(self, dataset_dir: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset benchmark loader with dataset directory.\n",
    "\n",
    "        :param dataset_dir: Path where datasets are stored.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "\n",
    "        # Categorizing datasets based on tasks\n",
    "        self.task_datasets = {\n",
    "            \"short_term_forecast\": [],\n",
    "            \"long_term_forecast\": [\n",
    "                \"electricity\",\n",
    "                \"exchange_rate\",\n",
    "                \"weather\",\n",
    "                \"illness\",\n",
    "                \"ETT-small\",\n",
    "            ],  # \"traffic\",\n",
    "            \"classification\": [\n",
    "                \"EthanolConcentration\",\n",
    "                \"FaceDetection\",\n",
    "                \"Handwriting\",\n",
    "                \"JapaneseVowels\",\n",
    "                \"PEMS-SF\",\n",
    "                \"SelfRegulationSCP1\",\n",
    "                \"SelfRegulationSCP2\",\n",
    "                \"SpokenArabicDigits\",\n",
    "                \"UWaveGestureLibrary\",\n",
    "            ],\n",
    "            \"anomaly_detection\": [\"MSL\", \"PSM\", \"SMD\", \"SMAP\", \"SWaT\"],\n",
    "        }\n",
    "\n",
    "    def to_long_format(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert dataset to long format with columns [unique_id, ds, y].\n",
    "\n",
    "        :param df: Original dataset\n",
    "        :param dataset_name: Name of the dataset\n",
    "        :return: Reformatted DataFrame\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        df = df.melt(id_vars=[\"index\"], var_name=\"ds\", value_name=\"y\")\n",
    "        df.rename(columns={\"index\": \"unique_id\"}, inplace=True)\n",
    "        df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "        return df\n",
    "\n",
    "    def load_dataset(self, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a dataset from the local directory and return it in long format.\n",
    "\n",
    "        :param dataset_name: Name of the dataset to load.\n",
    "        :return: DataFrame with columns [unique_id, ds, y].\n",
    "        \"\"\"\n",
    "        dataset_path = os.path.join(self.dataset_dir, dataset_name)\n",
    "\n",
    "        try:\n",
    "            # CSV-based datasets (Forecasting)\n",
    "            if dataset_name in [\n",
    "                \"electricity\",\n",
    "                \"exchange_rate\",\n",
    "                \"traffic\",\n",
    "                \"weather\",\n",
    "                \"illness\",\n",
    "                \"ETT-small\",\n",
    "            ]:\n",
    "                csv_file = os.path.join(dataset_path, os.listdir(dataset_path)[0])\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df.rename(columns={\"date\": \"ds\"}, inplace=True)\n",
    "                df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "                df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "                df[\"ds\"] = pd.to_datetime(df.ds)\n",
    "                return df\n",
    "\n",
    "            # .TS file-based datasets (Classification)\n",
    "            elif dataset_name in self.task_datasets[\"classification\"]:\n",
    "                train_file = os.path.join(dataset_path, f\"{dataset_name}_TRAIN.ts\")\n",
    "                test_file = os.path.join(dataset_path, f\"{dataset_name}_TEST.ts\")\n",
    "\n",
    "                df_train, y_train = load_from_tsfile_to_dataframe(train_file)\n",
    "                df_test, y_test = load_from_tsfile_to_dataframe(test_file)\n",
    "\n",
    "                df_train[\"label\"] = y_train\n",
    "                df_test[\"label\"] = y_test\n",
    "\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "                return df  # Keeping label column for classification\n",
    "\n",
    "            # .NPY file-based datasets (Anomaly Detection)\n",
    "            elif dataset_name in [\"MSL\", \"SMD\", \"SMAP\"]:\n",
    "                train_file = os.path.join(dataset_path, f\"{dataset_name}_train.npy\")\n",
    "                test_file = os.path.join(dataset_path, f\"{dataset_name}_test.npy\")\n",
    "                df_train = pd.DataFrame(np.load(train_file))\n",
    "                df_test = pd.DataFrame(np.load(test_file))\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "                return self.to_long_format(df, dataset_name)\n",
    "\n",
    "            # .CSV-based anomaly detection (PSM, SWaT)\n",
    "            elif dataset_name in [\"PSM\", \"SWaT\"]:\n",
    "                train_file = os.path.join(dataset_path, \"train.csv\")\n",
    "                test_file = os.path.join(dataset_path, \"test.csv\")\n",
    "                df_train = pd.read_csv(train_file)\n",
    "                df_test = pd.read_csv(test_file)\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "                return self.to_long_format(df, dataset_name)\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {dataset_name} not found or not supported.\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_datasets(self, task: str) -> Generator[Tuple[str, pd.DataFrame], None, None]:\n",
    "        \"\"\"\n",
    "        Generator function to yield datasets for a specific task.\n",
    "\n",
    "        :param task: Task type ('short_term_forecast', 'long_term_forecast', 'classification', 'anomaly_detection').\n",
    "        :yield: Tuple of dataset name and DataFrame.\n",
    "        \"\"\"\n",
    "        if task not in self.task_datasets:\n",
    "            raise ValueError(\n",
    "                f\"Invalid task: {task}. Choose from {list(self.task_datasets.keys())}\"\n",
    "            )\n",
    "\n",
    "        for dataset_name in self.task_datasets[task]:\n",
    "            df = self.load_dataset(dataset_name)\n",
    "            if df is not None:\n",
    "                yield dataset_name, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62bf14f4-738a-4cb0-98ac-2a4da6b62c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955bcf00-4aaf-447c-bd5b-f50fc0c17da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EthanolConcentration\n",
      "Index(['dim_0', 'dim_1', 'dim_2', 'label'], dtype='object') (524, 4)\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "from ts.benchmark.tsdataset import TimeSeriesBenchmarkDataset\n",
    "\n",
    "# Instantiate and initialize the dataset benchmark loader with your custom data directory\n",
    "dataset_dir = \"data/\"\n",
    "tbd = TimeSeriesBenchmarkDataset(dataset_dir=dataset_dir)\n",
    "\n",
    "# Define the task to get datasets for this specific task type\n",
    "task = \"classification\"  # or short_term_forecasting, long_term_forecasting, anomaly_detection\n",
    "\n",
    "# Fetch all available datasets of the defined task from your custom data directory\n",
    "datasets = tbd.get_datasets(task)\n",
    "\n",
    "for dataset_name, df in datasets:\n",
    "    print(\"Processing\", dataset_name)\n",
    "    print(df.columns, df.shape)\n",
    "    break\n",
    "    # Preprocess and train your model using these time series datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
