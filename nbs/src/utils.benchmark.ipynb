{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8b0db-1c55-48d4-91ac-e3f3129ed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp benchmark.tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40770940-3395-4476-8a04-3cc70db4a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437867fa-479c-4596-9abb-df3feeb901cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TimeSeriesBenchmarkDataset:\n",
    "    def __init__(self, dataset_dir: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset benchmark loader with dataset directory.\n",
    "\n",
    "        :param dataset_dir: Path where datasets are stored.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "\n",
    "        # Categorizing datasets based on tasks\n",
    "        self.task_datasets = {\n",
    "            \"short_term_forecast\": [],\n",
    "            \"long_term_forecast\": [\n",
    "                \"electricity\",\n",
    "                \"exchange_rate\",\n",
    "                \"weather\",\n",
    "                \"illness\",\n",
    "                \"ETT-small\",\n",
    "            ],  # \"traffic\",\n",
    "            \"classification\": [\n",
    "                \"EthanolConcentration\",\n",
    "                \"FaceDetection\",\n",
    "                \"Handwriting\",\n",
    "                \"JapaneseVowels\",\n",
    "                \"PEMS-SF\",\n",
    "                \"SelfRegulationSCP1\",\n",
    "                \"SelfRegulationSCP2\",\n",
    "                \"SpokenArabicDigits\",\n",
    "                \"UWaveGestureLibrary\",\n",
    "            ],\n",
    "            \"anomaly_detection\": [\"MSL\", \"PSM\", \"SMD\", \"SMAP\", \"SWaT\"],\n",
    "        }\n",
    "\n",
    "    def to_long_format(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert dataset to long format with columns [unique_id, ds, y].\n",
    "\n",
    "        :param df: Original dataset\n",
    "        :param dataset_name: Name of the dataset\n",
    "        :return: Reformatted DataFrame\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        df = df.melt(id_vars=[\"index\"], var_name=\"ds\", value_name=\"y\")\n",
    "        df.rename(columns={\"index\": \"unique_id\"}, inplace=True)\n",
    "        df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "        return df\n",
    "\n",
    "    def load_dataset(self, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a dataset from the local directory and return it in long format.\n",
    "\n",
    "        :param dataset_name: Name of the dataset to load.\n",
    "        :return: DataFrame with columns [unique_id, ds, y].\n",
    "        \"\"\"\n",
    "        dataset_path = os.path.join(self.dataset_dir, dataset_name)\n",
    "\n",
    "        try:\n",
    "            # CSV-based datasets\n",
    "            if dataset_name in [\n",
    "                \"electricity\",\n",
    "                \"exchange_rate\",\n",
    "                \"traffic\",\n",
    "                \"weather\",\n",
    "                \"illness\",\n",
    "                \"ETT-small\",\n",
    "            ]:\n",
    "                csv_file = os.path.join(dataset_path, os.listdir(dataset_path)[0])\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df.rename(columns={\"date\": \"ds\"}, inplace=True)\n",
    "                df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "                df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "                df[\"ds\"] = pd.to_datetime(df.ds)\n",
    "                return df\n",
    "\n",
    "            # .TS file-based datasets (Classification)\n",
    "            elif dataset_name in [\n",
    "                \"EthanolConcentration\",\n",
    "                \"FaceDetection\",\n",
    "                \"Handwriting\",\n",
    "                \"JapaneseVowels\",\n",
    "                \"PEMS-SF\",\n",
    "                \"SelfRegulationSCP1\",\n",
    "                \"SelfRegulationSCP2\",\n",
    "                \"SpokenArabicDigits\",\n",
    "                \"UWaveGestureLibrary\",\n",
    "            ]:\n",
    "                train_file = os.path.join(dataset_path, f\"{dataset_name}_TRAIN.ts\")\n",
    "                test_file = os.path.join(dataset_path, f\"{dataset_name}_TEST.ts\")\n",
    "                df_train, _ = load_from_tsfile_to_dataframe(train_file)\n",
    "                df_test, _ = load_from_tsfile_to_dataframe(test_file)\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "                return self.to_long_format(df, dataset_name)\n",
    "\n",
    "            # .NPY file-based datasets (Anomaly Detection)\n",
    "            elif dataset_name in [\"MSL\", \"SMD\", \"SMAP\"]:\n",
    "                train_file = os.path.join(dataset_path, f\"{dataset_name}_train.npy\")\n",
    "                test_file = os.path.join(dataset_path, f\"{dataset_name}_test.npy\")\n",
    "                df_train = pd.DataFrame(np.load(train_file))\n",
    "                df_test = pd.DataFrame(np.load(test_file))\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "                return self.to_long_format(df, dataset_name)\n",
    "\n",
    "            # .CSV-based anomaly detection (PSM, SWaT)\n",
    "            elif dataset_name in [\"PSM\", \"SWaT\"]:\n",
    "                train_file = os.path.join(dataset_path, \"train.csv\")\n",
    "                test_file = os.path.join(dataset_path, \"test.csv\")\n",
    "                df_train = pd.read_csv(train_file)\n",
    "                df_test = pd.read_csv(test_file)\n",
    "                df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "                return self.to_long_format(df, dataset_name)\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {dataset_name} not found or not supported.\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_datasets(self, task: str) -> Generator[Tuple[str, pd.DataFrame], None, None]:\n",
    "        \"\"\"\n",
    "        Generator function to yield datasets for a specific task.\n",
    "\n",
    "        :param task: Task type ('short_term_forecast', 'long_term_forecast', 'classification', 'anomaly_detection').\n",
    "        :yield: Tuple of dataset name and DataFrame.\n",
    "        \"\"\"\n",
    "        if task not in self.task_datasets:\n",
    "            raise ValueError(\n",
    "                f\"Invalid task: {task}. Choose from {list(self.task_datasets.keys())}\"\n",
    "            )\n",
    "\n",
    "        for dataset_name in self.task_datasets[task]:\n",
    "            df = self.load_dataset(dataset_name)\n",
    "            if df is not None:\n",
    "                yield dataset_name, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46aaeee-874a-4eda-ae1a-bbd888459f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Example Usage\n",
    "benchmark = TimeSeriesBenchmark()\n",
    "\n",
    "# Get datasets for short-term forecasting\n",
    "for name, df in benchmark.get_datasets(\"long_term_forecast\"):\n",
    "    print(f\"Loaded {name}, shape: {df.shape}\")\n",
    "    print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
