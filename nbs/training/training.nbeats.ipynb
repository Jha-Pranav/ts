{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0a16c4-7c00-4ecf-8a9f-49d77133efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp train.nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95def744-67d4-414e-8ac2-b0afeaeb611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning.profilers import PyTorchProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6a5233-2df7-44f7-8e8a-4980bf1330e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from datasetsforecast.m3 import M3\n",
    "# from datasetsforecast.m4 import M4\n",
    "# from datasetsforecast.m5 import M5\n",
    "\n",
    "# df = pd.concat(\n",
    "#     [\n",
    "#         M3().load(\"../data\", group=\"Monthly\")[0],\n",
    "#         M4().load(\"../data\", group=\"Monthly\")[0],\n",
    "#         M4().load(\"../data\", group=\"Weekly\")[0],\n",
    "#         M4().load(\"../data\", group=\"Daily\")[0],\n",
    "#         M5().load(\"../data\")[0],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Ensure ds is a datetime object\n",
    "# df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"coerce\")\n",
    "\n",
    "# # Sort values\n",
    "# df.sort_values([\"unique_id\", \"ds\"], inplace=True)\n",
    "\n",
    "# # Convert ds to an integer based on sorted order within each unique_id\n",
    "# df[\"ds\"] = df.groupby(\"unique_id\")[\"ds\"].rank(method=\"dense\").astype(int)\n",
    "\n",
    "# # Save as parquet\n",
    "# df.to_parquet(\"mid-range-forecast-data-M3-4-5.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5d7f14-19bf-4ed4-9e61-baae59cb5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"mid-range-forecast-data-M3-4-5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffdbbbe-1ed7-4b20-adfe-1cc86f3a0ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83076"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unique_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b9ae21-ff35-4d7d-a525-fc9421c426ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38491/840638693.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby(\"unique_id\").apply(len).describe()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    83076.000000\n",
       "mean       836.093794\n",
       "std        880.332992\n",
       "min         60.000000\n",
       "25%        193.000000\n",
       "50%        335.000000\n",
       "75%       1557.000000\n",
       "max       9933.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"unique_id\").apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0eb07-7fc3-4aa5-86ea-020565c39e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef30af7b-b966-451a-9049-5468cafdc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts.models.nbeats import NBeatsG\n",
    "\n",
    "horizon = 12\n",
    "input_size = horizon * 5\n",
    "\n",
    "model = NBeatsG(input_size=input_size, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6f3c39-add5-4301-8f1c-906a5d474ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52007182-f8d6-4f28-9b5a-db6e7260c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ts.preprocess.dataloader import UnivariateTSDataModule\n",
    "\n",
    "batch_size = 512 * 10\n",
    "num_workers = 24\n",
    "step_size = 6\n",
    "\n",
    "ds = UnivariateTSDataModule(\n",
    "    df=df,\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    normalize=True,\n",
    "    scaler_type=\"minmax\",\n",
    "    split_type=\"vertical\",\n",
    "    step_size=step_size,\n",
    "    prefetch_factor=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee22850-d3e7-42aa-aa22-41153a93a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9094f4f-3cd7-4c42-93e9-8a887f1adfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/projects/ts/.venv/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WandbLogger\n\u001b[32m      4\u001b[39m wandb_logger = WandbLogger(\n\u001b[32m      5\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33mshortterm-ts-global-forecast\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel=NBeatsG.ds=M5\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mwandb_logger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m profiler = PyTorchProfiler(\n\u001b[32m     11\u001b[39m     profile_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Track GPU memory\u001b[39;00m\n\u001b[32m     12\u001b[39m     record_shapes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m     with_stack=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Track CPU memory (if supported)\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m trainer = pl.Trainer(\n\u001b[32m     17\u001b[39m     logger=wandb_logger,\n\u001b[32m     18\u001b[39m     max_epochs=\u001b[32m200\u001b[39m,  \u001b[38;5;66;03m# Short run for testing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# strategy=\"ddp_notebook\"\u001b[39;00m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:424\u001b[39m, in \u001b[36mWandbLogger.watch\u001b[39m\u001b[34m(self, model, log, log_freq, log_graph)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwatch\u001b[39m(\n\u001b[32m    422\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: nn.Module, log: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mgradients\u001b[39m\u001b[33m\"\u001b[39m, log_freq: \u001b[38;5;28mint\u001b[39m = \u001b[32m100\u001b[39m, log_graph: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    423\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:391\u001b[39m, in \u001b[36m_run_decorator._attach.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mcls\u001b[39m._is_attaching = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2836\u001b[39m, in \u001b[36mRun.watch\u001b[39m\u001b[34m(self, models, criterion, log, log_freq, idx, log_graph)\u001b[39m\n\u001b[32m   2801\u001b[39m \u001b[38;5;129m@_run_decorator\u001b[39m._attach\n\u001b[32m   2802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwatch\u001b[39m(\n\u001b[32m   2803\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2809\u001b[39m     log_graph: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2810\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2811\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Hooks into the given PyTorch model(s) to monitor gradients and the model's computational graph.\u001b[39;00m\n\u001b[32m   2812\u001b[39m \n\u001b[32m   2813\u001b[39m \u001b[33;03m    This function can track parameters, gradients, or both during training. It should be\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2834\u001b[39m \u001b[33;03m            of `torch.nn.Module`.\u001b[39;00m\n\u001b[32m   2835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2836\u001b[39m     \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msdk\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_watch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_watch.py:120\u001b[39m, in \u001b[36m_watch\u001b[39m\u001b[34m(run, models, criterion, log, log_freq, idx, log_graph)\u001b[39m\n\u001b[32m    113\u001b[39m     run._torch.add_log_gradients_hook(\n\u001b[32m    114\u001b[39m         model,\n\u001b[32m    115\u001b[39m         prefix=prefix,\n\u001b[32m    116\u001b[39m         log_freq=log_freq,\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_graph:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     graph = \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_torch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     graphs.append(graph)\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# NOTE: the graph is set in run.summary by hook_torch on the backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py:315\u001b[39m, in \u001b[36mTorchGraph.hook_torch\u001b[39m\u001b[34m(cls, model, criterion, graph_idx)\u001b[39m\n\u001b[32m    313\u001b[39m wandb.termlog(\u001b[33m\"\u001b[39m\u001b[33mlogging graph, to disable use `wandb.watch(log_graph=False)`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    314\u001b[39m graph = TorchGraph()\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_torch_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ts/.venv/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py:370\u001b[39m, in \u001b[36mTorchGraph.hook_torch_modules\u001b[39m\u001b[34m(self, module, criterion, prefix, graph_idx, parent)\u001b[39m\n\u001b[32m    368\u001b[39m graph = \u001b[38;5;28mself\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m_wandb_watch_called\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m module._wandb_watch_called:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    372\u001b[39m     )\n\u001b[32m    373\u001b[39m module._wandb_watch_called = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m criterion:\n",
      "\u001b[31mValueError\u001b[39m: You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code."
     ]
    }
   ],
   "source": [
    "# | export\n",
    "# Example trainer setup (without full NBeatsG for brevity)\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"shortterm-ts-global-forecast\",\n",
    "    name=f\"model=NBeatsG.ds=M5\",\n",
    ")\n",
    "wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "profiler = PyTorchProfiler(\n",
    "    profile_memory=True,  # Track GPU memory\n",
    "    record_shapes=True,\n",
    "    with_stack=True,  # Track CPU memory (if supported)\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=200,  # Short run for testing\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    # logger=TensorBoardLogger(\"logs\", name=\"nbeatsg_m5\"),\n",
    "    callbacks=[EarlyStopping(\"val_smape\", patience=10, verbose=False)],\n",
    "    # profiler=profiler,\n",
    "    accumulate_grad_batches=4,\n",
    "    # strategy=\"ddp_notebook\"\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.fit(model, ds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c02e443-0eaa-46bc-b4e1-14e0f527681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ts.preprocess.dataloader:M47808 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47809 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47810 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47811 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47812 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47849 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47850 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47851 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47853 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47856 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47857 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47858 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47859 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47860 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47861 - Series too short for windowing\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d895244a95247bb9591d4624c1f7642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  2.6837395125767216e-05   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mase         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  3.2463102428437196e-08   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_owa          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8823177814483643     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_smape         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.0286610908806324     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 2.6837395125767216e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mase        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 3.2463102428437196e-08  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_owa         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8823177814483643    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_smape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.0286610908806324    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_mase</td><td>▁</td></tr><tr><td>test_owa</td><td>▁</td></tr><tr><td>test_smape</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▅▁▆▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▄█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>val_mase</td><td>▆█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▃▃▂▃▂▂</td></tr><tr><td>val_owa</td><td>▆█▅▃▂▁▂▁▁▁▁▁▁▁▁▁▃▃▂▃▂▂</td></tr><tr><td>val_smape</td><td>▆█▅▃▂▁▂▁▁▁▁▁▁▁▁▁▃▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>test_loss</td><td>3e-05</td></tr><tr><td>test_mase</td><td>0.0</td></tr><tr><td>test_owa</td><td>0.88232</td></tr><tr><td>test_smape</td><td>0.02866</td></tr><tr><td>train_loss_epoch</td><td>4e-05</td></tr><tr><td>train_loss_step</td><td>0.0001</td></tr><tr><td>trainer/global_step</td><td>23188</td></tr><tr><td>val_loss</td><td>3e-05</td></tr><tr><td>val_mase</td><td>0.0</td></tr><tr><td>val_owa</td><td>0.88236</td></tr><tr><td>val_smape</td><td>0.02866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model=NBeatsG.ds=M5</strong> at: <a href='https://wandb.ai/pranav_jha/shortterm-ts-global-forecast/runs/yf24szaf' target=\"_blank\">https://wandb.ai/pranav_jha/shortterm-ts-global-forecast/runs/yf24szaf</a><br> View project at: <a href='https://wandb.ai/pranav_jha/shortterm-ts-global-forecast' target=\"_blank\">https://wandb.ai/pranav_jha/shortterm-ts-global-forecast</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250330_131124-yf24szaf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "trainer.test(model, ds)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f778af1-bfec-4a8b-9fa0-4f54dfe9e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9e349-3b71-43e8-aab4-067560f61ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# trainer.save_checkpoint(\"SHORT-TERM-FORECAST-MODEL-NBEATSG(60-12).ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88a68574-1058-4dd6-94a9-48592dbc96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f90b83a-6152-4a24-9458-9dfa582ae5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts.models.nbeats import NBeatsG\n",
    "\n",
    "# horizon = 12\n",
    "# input_size = horizon * 5\n",
    "\n",
    "# model = NBeatsG(input_size=input_size, horizon=horizon)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "model = NBeatsG.load_from_checkpoint(\"SHORT-TERM-FORECAST-MODEL-NBEATSG(60-12).ckpt\")\n",
    "\n",
    "# If needed, load it into a Trainer to resume training or inference\n",
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# trainer = Trainer()\n",
    "# trainer.validate(model,ds)  # Run validation\n",
    "# trainer.test(model,ds);  # Run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57c2c1b-3dae-494e-95d3-aceb798fbd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ts.preprocess.dataloader:M47808 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47809 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47810 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47811 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47812 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47849 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47850 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47851 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47853 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47856 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47857 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47858 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47859 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47860 - Series too short for windowing\n",
      "WARNING:ts.preprocess.dataloader:M47861 - Series too short for windowing\n"
     ]
    }
   ],
   "source": [
    "ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607695b1-f2eb-4049-a689-08346edfcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "mse_values = []\n",
    "\n",
    "for x, y in ds.test_dataloader():\n",
    "    y_hat = model(x)  # Get predictions\n",
    "    errors = torch.nn.functional.mse_loss(y_hat, y, reduction=\"none\")  # Compute per-sample MSE\n",
    "    mse_values.extend(errors.detach().view(-1).cpu().numpy())  # Detach, flatten & move to CPU\n",
    "    break  # Only process first batch\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_error = pd.DataFrame({\"MSE\": mse_values})\n",
    "\n",
    "# Create violin plot\n",
    "fig = px.violin(df_error, y=\"MSE\", box=True, points=\"all\", title=\"Distribution of MSE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6c34ea-050c-4f55-ae70-e0088d64de39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.149312e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.626208e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.115133e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.014260e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.002962e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61435</th>\n",
       "      <td>1.559591e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61436</th>\n",
       "      <td>4.726880e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61437</th>\n",
       "      <td>1.703628e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61438</th>\n",
       "      <td>1.761820e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61439</th>\n",
       "      <td>8.153424e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61440 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MSE\n",
       "0      1.149312e-04\n",
       "1      1.626208e-09\n",
       "2      2.115133e-05\n",
       "3      4.014260e-06\n",
       "4      1.002962e-05\n",
       "...             ...\n",
       "61435  1.559591e-05\n",
       "61436  4.726880e-06\n",
       "61437  1.703628e-05\n",
       "61438  1.761820e-06\n",
       "61439  8.153424e-06\n",
       "\n",
       "[61440 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02ad1a-ee07-45c1-9a07-45ae3d6b326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def forecast_and_plot_grid(model, data_module, num_series=6):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get all unique_ids from the DataFrame\n",
    "    unique_ids = data_module.df[\"unique_id\"].unique()\n",
    "\n",
    "    # Randomly select series (or use all if fewer available)\n",
    "    selected_ids = np.random.choice(\n",
    "        unique_ids, size=min(num_series, len(unique_ids)), replace=False\n",
    "    )\n",
    "\n",
    "    # Prepare test data for selected series\n",
    "    test_data = data_module.df[data_module.df[\"unique_id\"].isin(selected_ids)]\n",
    "    grouped = test_data.groupby(\"unique_id\")\n",
    "\n",
    "    # Create figure with 3x2 grid\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=2,\n",
    "        subplot_titles=[f\"Series: {uid}\" for uid in selected_ids],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.1,\n",
    "    )\n",
    "\n",
    "    # Perform forecasting and plotting\n",
    "    for i, unique_id in enumerate(selected_ids):\n",
    "        series_df = grouped.get_group(unique_id)\n",
    "        series = series_df[\"y\"].values  # Raw series\n",
    "        series_len = len(series)\n",
    "\n",
    "        if series_len < data_module.input_size + data_module.horizon:\n",
    "            print(f\"Skipping {unique_id}: too short for forecasting\")\n",
    "            continue\n",
    "\n",
    "        # MinMax scaling on the entire series\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Generate last input window for forecasting\n",
    "        last_input = series_scaled[\n",
    "            -data_module.input_size - data_module.horizon : -data_module.horizon\n",
    "        ]\n",
    "        x = torch.tensor(last_input, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "\n",
    "        # Forecast\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x).cpu().numpy().flatten()\n",
    "\n",
    "        # Use MinMax transformed series, y_hat is already in model scale\n",
    "        full_time_indices = series_df[\"ds\"].values  # Full series timestamps\n",
    "        forecast_time_indices = full_time_indices[\n",
    "            -data_module.horizon :\n",
    "        ]  # Last horizon timestamps\n",
    "\n",
    "        # Determine row and column (Plotly uses 1-based indexing)\n",
    "        row = (i // 2) + 1\n",
    "        col = (i % 2) + 1\n",
    "\n",
    "        # Plot MinMax transformed actual series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=full_time_indices,\n",
    "                y=series_scaled,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"blue\"),\n",
    "                name=f\"Actual {unique_id}\",\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        # Plot predicted values for the last horizon\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecast_time_indices,\n",
    "                y=y_hat,  # Directly using model output\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"red\", dash=\"dash\"),\n",
    "                name=f\"Predicted {unique_id}\",\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        width=800,\n",
    "        title_text=\"Forecasting: MinMax Scaled Series with Predictions (3x2 Grid)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Scaled Value (0-1)\")\n",
    "    fig.update_xaxes(title_text=\"Date\")\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "forecast_and_plot_grid(model, ds, num_series=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab518be-0289-4378-a312-4c7daa0bc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.validate(model,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4da66e05-82e6-4523-9674-a20676740648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ts.preprocess.dataloader.UnivariateTSDataModule at 0x7fde2c9e7320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bd293fc-4437-4ceb-b1f5-f056280dd1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdceabd-13ad-4ff0-a933-3731b2f85545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python uv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
