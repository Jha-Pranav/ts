# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/src/classification.cnnclassifer.ipynb.

# %% auto 0
__all__ = ['TimeSeriesImageDataset', 'TimeSeriesDataset', 'TimeSeriesDataModule', 'TSImageClassifier', 'compute_conv_params',
           'ChannelReducerAndDownscaler', 'TSNDTensorClassifier']

# %% ../../nbs/src/classification.cnnclassifer.ipynb 2
import os
import copy
import pandas as pd
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.optim as optim
import torchmetrics
from PIL import Image
from torch.utils.data import DataLoader, Dataset, Subset, random_split
from torchvision import models, transforms

# %% ../../nbs/src/classification.cnnclassifer.ipynb 3
class TimeSeriesImageDataset(Dataset):
    """Loads time series image data from .png files and corresponding labels from labels.json."""

    def __init__(self, data_dir, resize_shape=(350, 350), transform=None):
        self.data_dir = data_dir
        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(".png")]

        self.labels = self.load_labels()  # Load labels from labels.json
        self.resize_shape = resize_shape
        self.transform = transform if transform else self.default_transform()

    def load_labels(self):
        """Loads labels from a single JSON file."""
        import json

        labels_path = os.path.join(self.data_dir, "labels.json")
        with open(labels_path, "r") as f:
            return json.load(f)

    def default_transform(self):
        """Returns a default transformation pipeline including resizing."""
        return transforms.Compose(
            [
                transforms.Resize(self.resize_shape),
                transforms.ToTensor(),  # Converts to [0, 1] float tensor
            ]
        )

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.data_dir, self.image_files[idx])
        img = Image.open(img_path).convert("RGB")  # Convert to 3-channel RGB

        # Apply transformation
        if self.transform:
            img = self.transform(img)

        label = torch.tensor(self.labels[str(idx)], dtype=torch.long)  # Load label from JSON
        return img, label

# %% ../../nbs/src/classification.cnnclassifer.ipynb 4
class TimeSeriesDataset(Dataset):
    def __init__(self, data_dir="processed_data", transform=None):
        """
        PyTorch dataset to load time series transformed into image tensors.

        Args:
            data_dir (str): Directory containing the saved .pt files.
            transform (callable, optional): Optional transform to apply to images.
        """
        self.data_dir = data_dir
        self.transform = transform
        self.files = sorted([f for f in os.listdir(data_dir) if f.endswith(".pt")])

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = os.path.join(self.data_dir, self.files[idx])
        sample = torch.load(file_path)

        image, label = sample["image"], sample["label"]

        if self.transform:
            image = self.transform(image)  # Apply any transformations (e.g., normalization)

        return image, label

# %% ../../nbs/src/classification.cnnclassifer.ipynb 5
class TimeSeriesDataModule(pl.LightningDataModule):
    def __init__(
        self,
        data_dir="processed_data",
        batch_size=64,
        num_workers=4,
        val_split=0.1,
        test_split=0.1,
        resize_shape=(350, 350),
        transform = None
    ):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.val_split = val_split
        self.test_split = test_split
        self.resize_shape = resize_shape
        self.transform = transform


    def setup(self, stage=None):
        """Randomly split dataset into train, validation, and test sets."""

        # Load dataset WITHOUT transform initially
        full_dataset = TimeSeriesDataset(self.data_dir, transform=None)
        total_size = len(full_dataset)

        val_size = int(self.val_split * total_size)
        test_size = int(self.test_split * total_size)
        train_size = total_size - val_size - test_size

        # ✅ Randomly split dataset
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset,
            [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42),  # Ensure reproducibility
        )

        # ✅ Apply transforms ONLY to the train dataset
        self.train_dataset = copy.deepcopy(train_dataset)
        self.train_dataset.dataset.transform = self.transform  # Apply train-specific transforms

        self.val_dataset = val_dataset  # No transform
        self.test_dataset = test_dataset  # No transform


    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )

# %% ../../nbs/src/classification.cnnclassifer.ipynb 6
class TSImageClassifier(pl.LightningModule):
    def __init__(
        self,
        model_name="convnext_tiny",
        num_classes=10,
        hidden_feature=512,
        lr=1e-3,
        freeze_backbone=True,
    ):
        super().__init__()
        self.save_hyperparameters()

        # Load model
        self.pretrained_model = self._load_model(model_name)

        # Freeze backbone if required
        if freeze_backbone:
            for param in self.pretrained_model.parameters():
                param.requires_grad = False

        # Modify classifier for custom classes
        self._modify_classifier(hidden_feature, num_classes)

        # Loss function
        self.criterion = nn.CrossEntropyLoss()
        self.lr = lr

        # Metrics (computed per batch)
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes)
        self.f1_score = torchmetrics.F1Score(
            task="multiclass", num_classes=num_classes, average="macro"
        )
        self.auc = torchmetrics.AUROC(task="multiclass", num_classes=num_classes)

    def _load_model(self, model_name):
        """Load a pretrained model dynamically."""
        model_dict = {
            "convnext_tiny": models.convnext_tiny(weights="IMAGENET1K_V1"),
            "efficientnet_b0": models.efficientnet_b0(weights="IMAGENET1K_V1"),
            "swin_v2_s": models.swin_v2_s(weights="IMAGENET1K_V1"),
            "resnet50": models.resnet50(weights="IMAGENET1K_V1"),
        }
        if model_name not in model_dict:
            raise ValueError(
                f"Model '{model_name}' is not supported. Choose from {list(model_dict.keys())}."
            )
        return model_dict[model_name]

    def _modify_classifier(self, hidden_feature, num_classes):
        """Modify classifier head for different models."""
        if hasattr(self.pretrained_model, "classifier"):
            in_features = self.pretrained_model.classifier[-1].in_features
            self.pretrained_model.classifier[-1] = nn.Sequential(
                nn.Linear(in_features, hidden_feature),
                nn.ReLU(),
                nn.Dropout(p=0.3),
                # nn.Linear(hidden_feature, hidden_feature),
                # nn.ReLU(),
                # nn.Dropout(p=0.3),
                nn.Linear(hidden_feature, num_classes),
            )
        elif hasattr(self.pretrained_model, "fc"):
            in_features = self.pretrained_model.fc.in_features
            self.pretrained_model.fc = nn.Sequential(
                nn.Linear(in_features, hidden_feature),
                nn.ReLU(),
                nn.Dropout(p=0.3),
                # nn.Linear(hidden_feature, hidden_feature),
                # nn.ReLU(),
                # nn.Dropout(p=0.3),
                nn.Linear(hidden_feature, num_classes),
            )

    def forward(self, x):
        return self.pretrained_model(x)

    def compute_metrics(self, logits, y, prefix):
        """Compute Accuracy, F1 Score, and AUC for a given batch."""
        preds = torch.argmax(logits, dim=1)
        acc = self.accuracy(preds, y)
        f1 = self.f1_score(preds, y)
        auc = self.auc(logits, y)

        self.log_dict(
            {
                f"{prefix}_accuracy": acc,
                f"{prefix}_f1": f1,
                f"{prefix}_auc": auc,
            },
            prog_bar=True,
        )

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)

        # Log metrics
        self.log("train_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "train")

        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)

        # Log metrics
        self.log("val_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "val")

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)

        # Log metrics
        self.log("test_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "test")

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
        return [optimizer], [scheduler]

# %% ../../nbs/src/classification.cnnclassifer.ipynb 7
# Compute dynamic convolution parameters
def compute_conv_params(input_size, output_size):
    stride = input_size // output_size
    kernel_size = (stride * 2) if stride > 1 else 3
    padding = (kernel_size - stride) // 2
    return kernel_size, stride, padding


# Preprocessing: Channel Reduction & Spatial Downsampling
class ChannelReducerAndDownscaler(nn.Module):
    def __init__(self, in_channels=164, reduced_channels=3, input_size=500, output_size=250):
        super(ChannelReducerAndDownscaler, self).__init__()
        kernel_size, stride, padding = compute_conv_params(input_size, output_size)

        # Reduce Channels with BatchNorm
        self.channel_reducer = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, reduced_channels, kernel_size=1),
            nn.BatchNorm2d(reduced_channels),
            nn.ReLU(),
        )

        # Downscale Spatial Dimensions using Conv with LayerNorm (Batch too small for BatchNorm)
        self.spatial_downscaler = nn.Sequential(
            nn.Conv2d(reduced_channels, reduced_channels, kernel_size, stride, padding),
            # nn.LayerNorm([reduced_channels, output_size, output_size]),  # LayerNorm for stability
            nn.ReLU()
        )

    def forward(self, x):
        x = self.channel_reducer(x)
        x = self.spatial_downscaler(x)
        return x  # Output: (batch, channels, output_size, output_size)


# Combined Model: Preprocessing + Classification
class TSNDTensorClassifier(pl.LightningModule):
    def __init__(
        self,
        model_name="convnext_tiny",
        num_classes=10,
        hidden_feature=256,
        lr=1e-4,  # Lowered learning rate
        freeze_backbone=True,
        in_channels=164,
        reduced_channels=3,
        input_size=500,
        output_size=250,
    ):
        super().__init__()
        self.save_hyperparameters()

        # Add Preprocessing (Channel Reduction & Downsampling)
        self.preprocessor = ChannelReducerAndDownscaler(
            in_channels, reduced_channels, input_size, output_size
        )

        # Load Pretrained Model
        self.pretrained_model = self._load_model(model_name)

        # Partially freeze backbone (Unfreeze last 10 layers)
        if freeze_backbone:
            for param in list(self.pretrained_model.parameters()):
                param.requires_grad = False

        # Modify Classifier
        self._modify_classifier(hidden_feature, num_classes)

        # Loss and Metrics
        self.criterion = nn.CrossEntropyLoss()
        self.lr = lr
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes, average="macro")
        self.f1_score = torchmetrics.F1Score(task="multiclass", num_classes=num_classes, average="macro")
        self.auc = torchmetrics.AUROC(task="multiclass", num_classes=num_classes, average="macro")

        # Initialize Weights with Xavier
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)

    def _load_model(self, model_name):
        """Load a pretrained model dynamically."""
        model_dict = {
            "convnext_tiny": models.convnext_tiny(weights="IMAGENET1K_V1"),
            "efficientnet_b0": models.efficientnet_b0(weights="IMAGENET1K_V1"),
            "swin_v2_s": models.swin_v2_s(weights="IMAGENET1K_V1"),
            "resnet50": models.resnet50(weights="IMAGENET1K_V1"),
        }
        if model_name not in model_dict:
            raise ValueError(
                f"Unsupported model '{model_name}'. Choose from {list(model_dict.keys())}."
            )
        return model_dict[model_name]

    def _modify_classifier(self, hidden_feature, num_classes):
        """Modify classifier head for different models."""
        if hasattr(self.pretrained_model, "classifier"):
            in_features = self.pretrained_model.classifier[-1].in_features
            self.pretrained_model.classifier[-1] = nn.Sequential(
                nn.Linear(in_features, hidden_feature),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_feature),  # Added BatchNorm before classifier
                nn.Dropout(p=0.1),  # Reduced Dropout
                nn.Linear(hidden_feature, num_classes),
            )
        elif hasattr(self.pretrained_model, "fc"):
            in_features = self.pretrained_model.fc.in_features
            self.pretrained_model.fc = nn.Sequential(
                nn.Linear(in_features, hidden_feature),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_feature),  # Added BatchNorm before classifier
                nn.Dropout(p=0.1),  # Reduced Dropout
                nn.Linear(hidden_feature, num_classes),
            )

    def forward(self, x):
        x = self.preprocessor(x)  # Apply preprocessing first
        x = self.pretrained_model(x)  # Pass through classifier
        return x

    def compute_metrics(self, logits, y, prefix):
        preds = torch.argmax(logits, dim=1)
        acc, f1, auc = self.accuracy(preds, y), self.f1_score(preds, y), self.auc(logits, y)
        self.log_dict({f"{prefix}_accuracy": acc, f"{prefix}_f1": f1, f"{prefix}_auc": auc}, prog_bar=True)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.log("train_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "train")
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.log("val_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "val")

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.log("test_loss", loss, prog_bar=True)
        self.compute_metrics(logits, y, "test")

    def configure_optimizers(self):
        optimizer = optim.AdamW(self.parameters(), lr=self.lr)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)  # Increased T_max
        return [optimizer], [scheduler]
