# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models/model.nbeats.ipynb.

# %% auto 0
__all__ = ['device', 'Block', 'NBeatsG']

# %% ../../nbs/models/model.nbeats.ipynb 1
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from torch.utils.data import DataLoader, Dataset
from torchmetrics import SymmetricMeanAbsolutePercentageError

from ..commons.loss import MASE, OWA

torch.set_float32_matmul_precision("high")

# %% ../../nbs/models/model.nbeats.ipynb 2
device = "cuda" if torch.cuda.is_available() else "mps" if torch.mps.is_available() else "cpu"

# %% ../../nbs/models/model.nbeats.ipynb 7
class Block(nn.Module):
    def __init__(self, input_size, horizon, hidden_size, theta_size):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, theta_size),
        )
        self.backcast_basis = nn.Linear(theta_size, input_size, bias=False)
        self.forecast_basis = nn.Linear(theta_size, horizon, bias=False)

    def forward(self, x):
        theta = self.fc(x)
        backcast = self.backcast_basis(theta)
        forecast = self.forecast_basis(theta)
        return backcast, forecast

# %% ../../nbs/models/model.nbeats.ipynb 8
class NBeatsG(pl.LightningModule):
    def __init__(
        self,
        input_size,
        horizon,
        hidden_size=512,
        theta_size=225,
        num_stacks=30,
        blocks_per_stack=3,
        lr=1e-3,
    ):
        super().__init__()
        self.save_hyperparameters()
        self.input_size = input_size
        self.horizon = horizon
        self.theta_size = theta_size
        self.lr = lr

        self.stacks = nn.ModuleList()
        for _ in range(num_stacks):
            stack = nn.ModuleList(
                [
                    Block(input_size, horizon, hidden_size, theta_size)
                    for _ in range(blocks_per_stack)
                ]
            )
            self.stacks.append(stack)

        self.loss_fn = nn.MSELoss()

        self.smape = SymmetricMeanAbsolutePercentageError()
        self.mase = MASE(input_size, horizon)
        self.owa = OWA(self.smape, self.mase)
        # Lazy metric initialization

        self._metrics = None
        self.register_buffer("forecast_template", torch.zeros(1, horizon))

    def forward(self, x):
        forecast = self.forecast_template.expand(
            x.size(0), -1
        ).clone()  # Clone to allocate separate memory
        residual = x.clone()  # Avoid modifying input directly

        for stack in self.stacks:
            stack_forecast = torch.zeros_like(forecast)
            for block in stack:
                backcast, block_forecast = block(residual)
                residual = residual - backcast  # Avoid in-place subtraction
                stack_forecast = stack_forecast + block_forecast  # Avoid in-place addition
            forecast = forecast + stack_forecast  # Avoid in-place addition

        return forecast

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss, on_epoch=True, prog_bar=True)

        # Update metrics
        self.smape.update(y_hat, y)
        self.mase.update(y_hat, y, x)
        self.owa.update(y_hat, y, x)

    def on_validation_epoch_end(self):
        # Compute and log metrics
        val_smape = self.smape.compute()
        val_mase = self.mase.compute()
        val_owa = self.owa.compute()

        self.log("val_smape", val_smape, on_epoch=True, prog_bar=False)
        self.log("val_mase", val_mase, on_epoch=True, prog_bar=False)
        self.log("val_owa", val_owa, on_epoch=True, prog_bar=False)

        # Reset metrics
        self.smape.reset()
        self.mase.reset()
        self.owa.reset()

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("test_loss", loss, on_epoch=True)

        self.smape.update(y_hat, y)
        self.mase.update(y_hat, y, x)
        self.owa.update(y_hat, y, x)

    def on_test_epoch_end(self):
        test_smape = self.smape.compute()
        test_mase = self.mase.compute()
        test_owa = self.owa.compute()

        self.log("test_smape", test_smape, on_epoch=True, prog_bar=True)
        self.log("test_mase", test_mase, on_epoch=True, prog_bar=True)
        self.log("test_owa", test_owa, on_epoch=True, prog_bar=True)

        self.smape.reset()
        self.mase.reset()
        self.owa.reset()

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=self.lr)
        scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)
        return [optimizer], [{"scheduler": scheduler, "interval": "epoch"}]
