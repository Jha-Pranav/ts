# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/src/model.baseline.ipynb.

# %% auto 0
__all__ = ['device', 'LTSF']

# %% ../../nbs/src/model.baseline.ipynb 1
import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.optim import Adam

# %% ../../nbs/src/model.baseline.ipynb 3
device = "cuda" if torch.cuda.is_available() else "cpu"
torch._dynamo.config.suppress_errors = True

# %% ../../nbs/src/model.baseline.ipynb 5
class LTSF(pl.LightningModule):
    def __init__(self, in_features, out_features, lr=1e-3):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features, bias=True)
        self.loss = nn.HuberLoss()
        self.lr = lr

        # Initialize weights for stability
        nn.init.constant_(self.linear.weight, 1.0 / in_features)
        nn.init.constant_(self.linear.bias, 0.0)

    def forward(self, x):
        return self.linear(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        x, y = x.to(self.device), y.to(self.device)
        y_pred = self(x)
        loss = self.loss(y_pred, y)
        # self.log("train_loss", loss, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x)
        loss = self.loss(y_pred, y)
        self.log("val_loss", loss, prog_bar=True, logger=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x)
        loss = self.loss(y_pred, y)
        self.log("test_loss", loss, prog_bar=True, logger=True)

    def configure_optimizers(self):
        return Adam(self.parameters(), lr=self.lr)
