# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/utils/preprocess.bigdata.dataloader.ipynb.

# %% auto 0
__all__ = ['logger', 'device', 'TSPreprocessor', 'UnivariateTSDataset', 'UnivariateTSDataModule']

# %% ../../../nbs/utils/preprocess.bigdata.dataloader.ipynb 1
import gc
import logging
import os
import pickle
from pathlib import Path
from typing import Optional

import h5py
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import ray
import torch
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from torch.utils.data import DataLoader, Dataset
from tqdm.notebook import tqdm

logger = logging.getLogger(__name__)

device = "cuda" if torch.cuda.is_available() else "cpu"
torch._dynamo.config.suppress_errors = True


class TSPreprocessor:
    def __init__(
        self,
        df,
        input_size,
        horizon,
        target_col="y_scaled",
        train_split=0.7,
        val_split=0.15,
        split_type="horizontal",
        step_size=1,
        cache_dir=".",
        use_cache=True,
        experiment_name="default_experiment",
        num_workers=4,
        chunk_size=1000,
        adaptive_step=False,
        downsample_factor=1,
        large_dataset=True,
        save_by_window_count=False,
        max_windows_per_file=2000,
        batch_size=200,
    ):
        logger.info("Initializing TSPreprocessor")
        self.df = df
        self.target_col = target_col if target_col in df.columns else "y"
        self.input_size = input_size
        self.horizon = horizon
        self.train_split = train_split
        self.val_split = val_split
        self.split_type = split_type
        self.step_size = step_size
        self.cache_dir = Path(cache_dir) / experiment_name
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        self.use_cache = use_cache
        self.num_workers = min(num_workers, os.cpu_count() or 4)
        self.experiment_name = experiment_name
        self.chunk_size = chunk_size
        self.adaptive_step = adaptive_step
        self.downsample_factor = downsample_factor
        self.large_dataset = large_dataset
        self.save_by_window_count = save_by_window_count
        self.max_windows_per_file = max_windows_per_file
        self.batch_size = batch_size

        if not 0 < train_split + val_split <= 1:
            raise ValueError("train_split + val_split must be between 0 and 1")
        if step_size < 1:
            raise ValueError("step_size must be >= 1")
        if downsample_factor < 1:
            raise ValueError("downsample_factor must be >= 1")
        if max_windows_per_file < 1:
            raise ValueError("max_windows_per_file must be >= 1")
        if batch_size < 1:
            raise ValueError("batch_size must be >= 1")

        # Initialize Ray
        if not ray.is_initialized():
            ray.init(num_cpus=self.num_workers, ignore_reinit_error=True)

        self.train_windows, self.val_windows, self.test_windows = self._process_data()

        if len(self.train_windows) == 0:
            logger.error(
                "No training windows generated. Check input_size, horizon, or data lengths."
            )
            raise ValueError(
                "Training dataset is empty. Ensure series lengths are sufficient for input_size + horizon."
            )

        del self.df
        gc.collect()

    def _generate_windows_in_memory(self, series):
        series_len = len(series)
        if series_len < self.input_size + self.horizon:
            return []

        max_idx = series_len - self.input_size - self.horizon + 1
        if max_idx <= 0:
            return []

        window_starts = np.arange(0, max_idx, self.step_size, dtype=np.int32)
        window_ends = window_starts + self.input_size
        horizon_ends = window_ends + self.horizon

        valid_windows = horizon_ends <= series_len
        window_starts = window_starts[valid_windows]
        window_ends = window_ends[valid_windows]
        horizon_ends = horizon_ends[valid_windows]

        x_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=self.input_size)[
            window_starts
        ]
        y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])

        return list(zip(x_windows, y_windows))

    @staticmethod
    def _generate_windows_optimized(
        series,
        unique_id,
        split,
        window_buffer=None,
        chunk_size=None,
        input_size=None,
        horizon=None,
        step_size=None,
        adaptive_step=False,
        cache_dir=None,
    ):
        series_len = len(series)
        if series_len < input_size + horizon:
            logger.debug(
                f"[{unique_id}] Skipping series for {split} split (length={series_len} < {input_size + horizon})"
            )
            return []

        max_idx = series_len - input_size - horizon + 1
        if max_idx <= 0:
            logger.debug(f"[{unique_id}] No valid windows for {split} split (max_idx={max_idx})")
            return []

        step_size = step_size
        if adaptive_step:
            step_size = max(1, series_len // 1000)

        chunk_size = chunk_size or max(100, min(1000, series_len // 10))

        h5_file = Path(cache_dir) / f"{unique_id}_{split}.h5"
        window_count = 0
        with h5py.File(h5_file, "a") as f:
            x_dset = (
                f["x"]
                if "x" in f
                else f.create_dataset(
                    "x",
                    shape=(0, input_size),
                    maxshape=(None, input_size),
                    dtype=np.float32,
                    compression="lzf",
                )
            )
            y_dset = (
                f["y"]
                if "y" in f
                else f.create_dataset(
                    "y",
                    shape=(0, horizon),
                    maxshape=(None, horizon),
                    dtype=np.float32,
                    compression="lzf",
                )
            )

            for start in range(0, max_idx, chunk_size * step_size):
                end = min(start + chunk_size * step_size, max_idx)
                window_starts = np.arange(start, end, step_size, dtype=np.int32)
                window_ends = window_starts + input_size
                horizon_ends = window_ends + horizon

                valid_windows = horizon_ends <= series_len
                window_starts = window_starts[valid_windows]
                window_ends = window_ends[valid_windows]
                horizon_ends = horizon_ends[valid_windows]

                x_windows = np.lib.stride_tricks.sliding_window_view(
                    series, window_shape=input_size
                )[window_starts]
                y_windows = np.stack([series[we:he] for we, he in zip(window_ends, horizon_ends)])

                x_dset.resize(x_dset.shape[0] + len(x_windows), axis=0)
                y_dset.resize(y_dset.shape[0] + len(y_windows), axis=0)
                x_dset[-len(x_windows) :] = x_windows
                y_dset[-len(y_windows) :] = y_windows
                window_count += len(x_windows)
                logger.debug(
                    f"[{unique_id}] Saved {len(x_windows)} windows to {h5_file} for {split} split"
                )

                del x_windows, y_windows
                gc.collect()

        logger.debug(
            f"[{unique_id}] Total {window_count} windows saved to {h5_file} for {split} split"
        )
        return [h5_file]

    @staticmethod
    @ray.remote
    def _save_buffered_windows(window_buffer, file_idx, split, cache_dir, input_size, horizon):
        if not window_buffer:
            logger.debug(f"No windows to save for {split} split, file_idx={file_idx}")
            return file_idx

        h5_file = Path(cache_dir) / f"windows_{file_idx}_{split}.h5"
        with h5py.File(h5_file, "a") as f:
            x_dset = (
                f["x"]
                if "x" in f
                else f.create_dataset(
                    "x",
                    shape=(0, input_size),
                    maxshape=(None, input_size),
                    dtype=np.float32,
                    compression="lzf",
                )
            )
            y_dset = (
                f["y"]
                if "y" in f
                else f.create_dataset(
                    "y",
                    shape=(0, horizon),
                    maxshape=(None, horizon),
                    dtype=np.float32,
                    compression="lzf",
                )
            )

            x_windows, y_windows = zip(*window_buffer)
            x_windows = np.stack(x_windows)
            y_windows = np.stack(y_windows)

            x_dset.resize(x_dset.shape[0] + len(x_windows), axis=0)
            y_dset.resize(y_dset.shape[0] + len(y_windows), axis=0)
            x_dset[-len(x_windows) :] = x_windows
            y_dset[-len(y_windows) :] = y_windows
            logger.debug(f"Saved {len(x_windows)} windows to {h5_file} for {split} split")

        del window_buffer[:]
        gc.collect()
        return file_idx + 1

    def _process_one_series_in_memory(self, unique_id_group):
        unique_id, group = unique_id_group
        series = group[self.target_col].values.astype(np.float32)

        windows = self._generate_windows_in_memory(series)
        del series
        gc.collect()

        if not windows:
            logger.warning(f"[{unique_id}] Series too short to generate any windows")
            return [], [], [], unique_id

        if self.split_type == "horizontal":
            num_windows = len(windows)
            train_end = int(num_windows * self.train_split)
            val_end = train_end + int(num_windows * self.val_split)
            return (windows[:train_end], windows[train_end:val_end], windows[val_end:], unique_id)
        else:
            return windows, [], [], unique_id

    @staticmethod
    @ray.remote(num_cpus=1)
    def _process_batch_optimized(
        unique_id_groups,
        window_buffers,
        file_indices,
        cache_dir,
        input_size,
        horizon,
        step_size,
        adaptive_step,
        downsample_factor,
        chunk_size,
        max_windows_per_file,
        split_type,
        train_split,
        val_split,
        target_col,
    ):
        results = []
        for unique_id_group in unique_id_groups:
            unique_id, group = unique_id_group
            series = group[target_col].values.astype(np.float32)

            if len(series) < input_size + horizon:
                logger.debug(
                    f"[{unique_id}] Skipping series (length={len(series)} < {input_size + horizon})"
                )
                continue

            if downsample_factor > 1:
                series = series[::downsample_factor]

            step_size = step_size
            if adaptive_step:
                step_size = max(1, len(series) // 1000)

            if split_type == "horizontal":
                series_len = len(series)
                num_windows = max(0, (series_len - input_size - horizon + 1) // step_size)
                train_end = int(num_windows * train_split)
                val_end = train_end + int(num_windows * val_split)

                result = []
                for split, start, end in [
                    ("train", 0, train_end * step_size),
                    ("val", train_end * step_size, val_end * step_size),
                    ("test", val_end * step_size, series_len),
                ]:
                    h5_files = TSPreprocessor._generate_windows_optimized(
                        series[start:end],
                        unique_id,
                        split,
                        None,
                        chunk_size,
                        input_size,
                        horizon,
                        step_size,
                        adaptive_step,
                        cache_dir,
                    )
                    if h5_files:
                        result.append((h5_files[0], unique_id, split))
                    else:
                        result.append((None, unique_id, split))
                results.append((result, window_buffers, file_indices))
            else:
                h5_files = TSPreprocessor._generate_windows_optimized(
                    series,
                    unique_id,
                    "all",
                    None,
                    chunk_size,
                    input_size,
                    horizon,
                    step_size,
                    adaptive_step,
                    cache_dir,
                )
                result = []
                if h5_files:
                    result.append((h5_files[0], unique_id, "all"))
                else:
                    result.append((None, unique_id, "all"))
                results.append((result, window_buffers, file_indices))
        return results

    def _process_data(self):
        logger.info("Processing data for all unique_ids")
        cache_file = self.cache_dir / "preprocessed_windows.pkl"

        if self.use_cache and cache_file.exists():
            logger.info("Loading preprocessed windows from cache")
            with open(cache_file, "rb") as f:
                data = pickle.load(f)
            return data["train_windows"], data["val_windows"], data["test_windows"]

        grouped = list(self.df.groupby("unique_id"))
        logger.info(f"Found {len(grouped)} unique IDs")

        # Pre-filter series that are too short
        valid_grouped = []
        lengths = []
        for unique_id, group in grouped:
            series_len = len(group[self.target_col])
            lengths.append(series_len)
            if series_len >= self.input_size + self.horizon:
                valid_grouped.append((unique_id, group))
            else:
                logger.debug(
                    f"[{unique_id}] Excluded: series length {series_len} < {self.input_size + self.horizon}"
                )
        logger.info(f"After filtering, {len(valid_grouped)} valid series remain")
        if lengths:
            lengths = np.array(lengths)
            logger.info(
                f"Series length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.2f}, "
                f"median={np.median(lengths):.2f}, too_short={np.sum(lengths < self.input_size + self.horizon)}"
            )

        if not valid_grouped:
            logger.error("No series are long enough to generate windows.")
            raise ValueError("No valid series found for window generation.")

        train_windows, val_windows, test_windows = [], [], []
        train_ids, val_ids, test_ids = [], [], []

        if self.split_type == "vertical":
            logger.info("Applying vertical split")
            unique_ids = [unique_id for unique_id, _ in valid_grouped]
            np.random.shuffle(unique_ids)
            total_series = len(unique_ids)
            train_end = int(total_series * self.train_split)
            val_end = train_end + int(total_series * self.val_split)

            train_ids = unique_ids[:train_end]
            val_ids = unique_ids[train_end:val_end]
            test_ids = unique_ids[val_end:]
            logger.info(
                f"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}"
            )

        if self.large_dataset:
            window_buffers = {"train": [], "val": [], "test": [], "all": []}
            file_indices = {"train": 0, "val": 0, "test": 0, "all": 0}

            # Batch unique IDs
            batches = [
                valid_grouped[i : i + self.batch_size]
                for i in range(0, len(valid_grouped), self.batch_size)
            ]
            logger.debug(f"Created {len(batches)} batches")

            # Submit Ray tasks
            futures = [
                TSPreprocessor._process_batch_optimized.remote(
                    batch,
                    window_buffers.copy(),
                    file_indices.copy(),
                    self.cache_dir,
                    self.input_size,
                    self.horizon,
                    self.step_size,
                    self.adaptive_step,
                    self.downsample_factor,
                    self.chunk_size,
                    self.max_windows_per_file,
                    self.split_type,
                    self.train_split,
                    self.val_split,
                    self.target_col,
                )
                for batch in batches
            ]
            logger.debug(f"Submitted {len(futures)} Ray tasks")

            # Collect results
            pbar = tqdm(total=len(futures), desc="Processing batches", dynamic_ncols=True)
            results = []
            while futures:
                done, futures = ray.wait(futures, num_returns=min(len(futures), 1))
                batch_results = ray.get(done)[0]
                results.extend(batch_results)
                pbar.update(len(done))
            pbar.close()

            if self.save_by_window_count:
                futures = []
                for split in window_buffers:
                    buffer = window_buffers[split]
                    file_idx = file_indices[split]
                    while buffer:
                        futures.append(
                            TSPreprocessor._save_buffered_windows.remote(
                                buffer[: self.max_windows_per_file],
                                file_idx,
                                split,
                                self.cache_dir,
                                self.input_size,
                                self.horizon,
                            )
                        )
                        h5_file = self.cache_dir / f"windows_{file_idx}_{split}.h5"
                        if split == "train" or (self.split_type == "vertical" and split == "all"):
                            train_windows.append((h5_file, None))
                        elif split == "val":
                            val_windows.append((h5_file, None))
                        elif split == "test":
                            test_windows.append((h5_file, None))
                        buffer = buffer[self.max_windows_per_file :]
                        file_idx += 1
                    file_indices[split] = file_idx

                pbar = tqdm(total=len(futures), desc="Saving windows", dynamic_ncols=True)
                while futures:
                    done, futures = ray.wait(futures, num_returns=min(len(futures), 1))
                    for file_idx in ray.get(done):
                        file_indices[split] = max(file_indices[split], file_idx)
                    pbar.update(len(done))
                pbar.close()

            for result, _, _ in results:
                for h5_file, unique_id, split in result:
                    if h5_file:
                        if self.split_type == "horizontal":
                            if split == "train":
                                train_windows.append((h5_file, unique_id))
                            elif split == "val":
                                val_windows.append((h5_file, unique_id))
                            elif split == "test":
                                test_windows.append((h5_file, unique_id))
                        else:
                            if unique_id in train_ids:
                                train_windows.append((h5_file, unique_id))
                            elif unique_id in val_ids:
                                val_windows.append((h5_file, unique_id))
                            elif unique_id in test_ids:
                                test_windows.append((h5_file, unique_id))

        else:
            from concurrent.futures import ProcessPoolExecutor

            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
                results = list(
                    tqdm(
                        executor.map(self._process_one_series_in_memory, valid_grouped),
                        total=len(valid_grouped),
                        desc="Processing series",
                    )
                )

            for train_w, val_w, test_w, unique_id in results:
                if self.split_type == "horizontal":
                    train_windows.extend(train_w)
                    val_windows.extend(val_w)
                    test_windows.extend(test_w)
                else:
                    if unique_id in train_ids:
                        train_windows.extend(train_w)
                    elif unique_id in val_ids:
                        val_windows.extend(val_w)
                    elif unique_id in test_ids:
                        test_windows.extend(test_w)

        train_windows = np.array(train_windows, dtype=object)
        val_windows = np.array(val_windows, dtype=object)
        test_windows = np.array(test_windows, dtype=object)

        logger.info(
            f"Train windows: {len(train_windows)}, Val windows: {len(val_windows)}, Test windows: {len(test_windows)}"
        )

        if len(train_windows) == 0:
            logger.error(
                "No training windows generated after processing. Check split_type, train_split, or data."
            )
            raise ValueError("Training dataset is empty. Adjust parameters or verify data.")

        if self.use_cache:
            logger.info("Saving preprocessed windows metadata to cache")
            with open(cache_file, "wb") as f:
                pickle.dump(
                    {
                        "train_windows": train_windows,
                        "val_windows": val_windows,
                        "test_windows": test_windows,
                    },
                    f,
                )

        return train_windows, val_windows, test_windows


class UnivariateTSDataset(Dataset):
    def __init__(self, windows, device: str = None, large_dataset: bool = True):
        logger.info("Initializing UnivariateTSDataset")
        self.windows = windows
        self.device = device
        self.large_dataset = large_dataset
        if large_dataset:
            self.h5_files = {}
        else:
            self.x = np.stack([w[0] for w in windows], axis=0).astype(np.float32)
            self.y = np.stack([w[1] for w in windows], axis=0).astype(np.float32)
            if device:
                logger.info("Preloading tensors to device: %s", device)
                self.x_tensor = torch.from_numpy(self.x).to(device)
                self.y_tensor = torch.from_numpy(self.y).to(device)
            else:
                self.x_tensor = self.y_tensor = None

    def __len__(self):
        if self.large_dataset:
            total_len = 0
            for h5_file, _ in self.windows:
                with h5py.File(h5_file, "r") as f:
                    total_len += f["x"].shape[0]
            return total_len
        return len(self.x)

    def __getitem__(self, idx):
        if self.large_dataset:
            current_idx = 0
            for h5_file, _ in self.windows:
                with h5py.File(h5_file, "r") as f:  # Context manager closes file
                    num_windows = f["x"].shape[0]
                    if current_idx + num_windows > idx:
                        local_idx = idx - current_idx
                        x = f["x"][local_idx]
                        y = f["y"][local_idx]
                        if self.device:
                            return torch.from_numpy(x).to(self.device), torch.from_numpy(y).to(self.device)
                        return torch.from_numpy(x), torch.from_numpy(y)
                    current_idx += num_windows
            raise IndexError("Index out of range")
        else:
            if self.device:
                return self.x_tensor[idx], self.y_tensor[idx]
            return torch.from_numpy(self.x[idx]), torch.from_numpy(self.y[idx])

    def __del__(self):
        if self.large_dataset:
            for h5_file in self.h5_files.values():
                h5_file.close()


class UnivariateTSDataModule(pl.LightningDataModule):
    def __init__(
        self,
        preprocessor: TSPreprocessor,
        batch_size=32,
        num_workers=8,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True,
        gpu_preload=False,
        experiment_name="default_experiment",
    ):
        logger.info("Initializing UnivariateTSDataModule")
        super().__init__()
        self.save_hyperparameters(ignore=["preprocessor"])
        self.preprocessor = preprocessor
        self.batch_size = batch_size
        self.num_workers = min(num_workers, torch.get_num_threads())
        self.pin_memory = pin_memory and not gpu_preload
        self.prefetch_factor = prefetch_factor
        self.persistent_workers = persistent_workers
        self.gpu_preload = gpu_preload
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.experiment_name = experiment_name

    def setup(self, stage: str = None):
        logger.info(f"Setting up datamodule for stage: {stage}")
        if stage in ("fit", None):
            self.train_dataset = UnivariateTSDataset(
                self.preprocessor.train_windows,
                device=self.device if self.gpu_preload else None,
                large_dataset=self.preprocessor.large_dataset,
            )
            self.val_dataset = UnivariateTSDataset(
                self.preprocessor.val_windows,
                device=self.device if self.gpu_preload else None,
                large_dataset=self.preprocessor.large_dataset,
            )
        if stage in ("validate", None):
            self.val_dataset = UnivariateTSDataset(
                self.preprocessor.val_windows,
                device=self.device if self.gpu_preload else None,
                large_dataset=self.preprocessor.large_dataset,
            )
        if stage in ("test", None):
            self.test_dataset = UnivariateTSDataset(
                self.preprocessor.test_windows,
                device=self.device if self.gpu_preload else None,
                large_dataset=self.preprocessor.large_dataset,
            )

    def _create_dataloader(self, dataset, shuffle=False):
        logger.info("Creating dataloader")
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            prefetch_factor=self.prefetch_factor,
            persistent_workers=self.persistent_workers,
            drop_last=shuffle,
        )

    def train_dataloader(self):
        logger.info("Getting train dataloader")
        return self._create_dataloader(self.train_dataset, shuffle=True)

    def val_dataloader(self):
        logger.info("Getting val dataloader")
        return self._create_dataloader(self.val_dataset)

    def test_dataloader(self):
        logger.info("Getting test dataloader")
        return self._create_dataloader(self.test_dataset)
