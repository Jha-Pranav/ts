# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/training/training.nbeats.ipynb.

# %% auto 0
__all__ = ['batch_size', 'num_workers', 'step_size', 'ds', 'wandb_logger', 'profiler', 'trainer']

# %% ../../nbs/training/training.nbeats.ipynb 8
import torch

# %% ../../nbs/training/training.nbeats.ipynb 9
from ..preprocess.dataloader import UnivariateTSDataModule

batch_size = 512 * 10
num_workers = 24
step_size = 6

ds = UnivariateTSDataModule(
    df=df,
    input_size=input_size,
    horizon=horizon,
    batch_size=batch_size,
    num_workers=num_workers,
    train_split=0.7,
    val_split=0.15,
    normalize=True,
    scaler_type="minmax",
    split_type="vertical",
    step_size=step_size,
    prefetch_factor=4,
)

# %% ../../nbs/training/training.nbeats.ipynb 10
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

# %% ../../nbs/training/training.nbeats.ipynb 11
# Example trainer setup (without full NBeatsG for brevity)
from lightning.pytorch.loggers import WandbLogger

wandb_logger = WandbLogger(
    project="shortterm-ts-global-forecast",
    name=f"model=NBeatsG.ds=M5",
)
wandb_logger.watch(model, log="all")

profiler = PyTorchProfiler(
    profile_memory=True,  # Track GPU memory
    record_shapes=True,
    with_stack=True,  # Track CPU memory (if supported)
)

trainer = pl.Trainer(
    logger=wandb_logger,
    max_epochs=200,  # Short run for testing
    accelerator="auto",
    precision="16-mixed",
    gradient_clip_val=1.0,
    # logger=TensorBoardLogger("logs", name="nbeatsg_m5"),
    callbacks=[EarlyStopping("val_smape", patience=10, verbose=False)],
    # profiler=profiler,
    accumulate_grad_batches=4,
    # strategy="ddp_notebook"
)


# trainer.fit(model, ds,)

# %% ../../nbs/training/training.nbeats.ipynb 12
trainer.test(model, ds)
wandb.finish()

# %% ../../nbs/training/training.nbeats.ipynb 14
# trainer.save_checkpoint("SHORT-TERM-FORECAST-MODEL-NBEATSG(60-12).ckpt")
