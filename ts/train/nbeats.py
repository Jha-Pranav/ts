# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/training/training.nbeats.ipynb.

# %% auto 0
__all__ = ['df', 'batch_size', 'num_workers', 'step_size', 'ds', 'profiler', 'trainer']

# %% ../../nbs/training/training.nbeats.ipynb 1
from datasetsforecast.m5 import M5

# , group="Monthly"
df = M5().load("../data")[0]
df.sort_values(["unique_id", "ds"], inplace=True)
df

# %% ../../nbs/training/training.nbeats.ipynb 5
from ..preprocess.dataloader import UnivariateTSDataModule

# %% ../../nbs/training/training.nbeats.ipynb 6
batch_size = 512*10
num_workers = 24
step_size = 6

ds = UnivariateTSDataModule(
    df=df,
    input_size=input_size,
    horizon=horizon,
    batch_size=batch_size,
    num_workers=num_workers,
    train_split=0.7,
    val_split=0.15,
    normalize=True,
    scaler_type="minmax",
    split_type="vertical",
    step_size=step_size,
)

# %% ../../nbs/training/training.nbeats.ipynb 7
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger


# %% ../../nbs/training/training.nbeats.ipynb 8
# Example trainer setup (without full NBeatsG for brevity)

import torch
from pytorch_lightning.profilers import PyTorchProfiler

torch.autograd.set_detect_anomaly(True)

profiler = PyTorchProfiler(
    profile_memory=True,  # Track GPU memory
    record_shapes=True,
    with_stack=True,  # Track CPU memory (if supported)
)

trainer = pl.Trainer(
    max_epochs=200,  # Short run for testing
    accelerator="auto",
    logger=TensorBoardLogger("logs", name="nbeatsg_m5"),
    callbacks=[EarlyStopping("val_loss", patience=15, verbose=False)],
    # profiler=profiler,
    accumulate_grad_batches=4,
    # strategy="ddp_notebook"
)


trainer.fit(model, ds)

# %% ../../nbs/training/training.nbeats.ipynb 10
trainer.test(model, ds);

# %% ../../nbs/training/training.nbeats.ipynb 12
trainer.save_checkpoint("M5-profile.ckpt")
